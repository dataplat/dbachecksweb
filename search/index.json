[{"content":"dbachecks v3 at Data Saturday Oslo On Saturday September 2nd 2023, will be Data Saturday Oslo in Oslo, Norway.\nFor more information about this free Saturday event, visit the official event page at DataSaturday0035 where you can sign up. Don\u0026rsquo;t wait, spaces are limited!\nI will be presenting a session with Jess Pomfret Blog Mastodon and Cláudio Silva Blog Twitter introducing dbachecks v3.\nabstract dbachecks v1 was released back in 2018, bringing together the DBA PowerShell magic of dbatools with pester, a PowerShell testing framework to create a powerful infrastructure validation tool.\ndbachecks is able to answer questions such as:\ncan I connect to all of my instances? was my last full backup with 7 days are my databases owned by the expected login am I using \u0026lsquo;page verify\u0026rsquo; to detect corruption are any of my certificates nearing expiration and many more\u0026hellip; v2 introduced the ability to store these results in a database and create a time series chart showing changes over time.\nBut now, we introduce v3!\ndbachecks v3 takes advantage of Pester v5 - a total rewrite of the tool enabled us to not only increase the performance of the module but position it to be able to grow and expand in a controlled and predictable way in the future.\nIn this session, we\u0026rsquo;ll talk about the changes we\u0026rsquo;ve made to the module, demo the new features and prove the massive performance increase using Profiler.\nCome and join us as we share the biggest release yet of the dbachecks module and learn how you can utilise it in your environments.\nFor more information about the free Saturday event, visit the official event page at DataSaturday0035. Don\u0026rsquo;t wait, spaces are limited!\nYou can also join Jess and I on Friday 1st September 2023 for a power-packed day of learning, networking, and enhancing your DBA skills.\nReady to take your DBA skills to the next level in a hybrid environment? Secure your spot now by registering at the Eventbrite page: Register Here.\n","date":"2023-07-14T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2023/datasaturdayoslo-session.jpeg","permalink":"https://dbachecks.io/blog/dbachecks-v3-at-data-saturday-oslo/","title":"dbachecks v3 at Data Saturday Oslo"},{"content":"I enjoy maintaining open source GitHub repositories such as dbachecks and ADSNotebook. I absolutely love it when people add more functionality to them.\nTo collaborate with a repository in GitHub you need to follow these steps\nFork the repository into your own GitHub Clone the repository to your local machine Create a new branch for your changes Make some changes and commit them with useful messages Push the changes to your repository Create a Pull Request from your repository back to the original one You will need to have git.exe available which you can download and install from https://git-scm.com/downloads if required\nFork the repository into your own GitHub A fork is a copy of the original repository. This allows you to make changes without affecting the original project. It does not get updated when the original project gets updated (We will talk about that in the next post) This enables you to code a new feature or a bug fix, test it locally and make sure it is working.\nLet’s take dbachecks as our example. Start by going to the project in GiHub. In this case the URL is https://github.com/dataplat/dbachecks You will see a Fork button at the top right of the page\nWhen you click the button the repository is copied into your own GitHub account\nThe page will open at https://github.com/YOURGITHUBUSERNAME/NameOfRepository in this case https://github.com/SQLDBAWithABeard/dbachecks You will be able to see that it is a fork of the original repository at the top of the page\nClone the repository to your local machine Forking the repository has created a remote repository stored on the GitHub servers. Now that the repository has been forked you need to clone it to your local machine to create a local repository so that you can start coding your amazing fix. When you have finished you can then sync it back to your remote repository ready for a Pull Request back to the original repository.\nIn your browser, at your remote repository that you just created (https://github.com/YOURGITHUBUSERNAME/NameOfRepository if you have closed the page) click on Clone or Download and then the icon to the right to copy the url\nYou can clone your repository in VS Code or Azure Data Studio by clicking F1 or CTRL + SHIFT + P in Windows or Linux and ⇧⌘P or F1 on a Mac\nthen start typing clone until you see Git:Clone and press enter or click\nPaste in the URL that you just copied and click enter. A dialog will open asking you to select a folder. This is the parent directory where your local repository will be created. The clone will create a directory for your repository so you do not need to. I suggest that you use a folder called GitHub or something similar to place all of the repositories that you are going to clone and create.\nWhen it has finished it will ask you if you wish to open the repository\nif you click Open it will close anything that you have already got opened and open the folder. If you click Add to Workspace it will add the folder to the workspace and leave everything you already had open as it was and surprisingly clicking Open in New Window will open the folder in a new instance of Visual Studio Code or Azure Data Studio!\nand you will also be able to see the local repository files on your computer\nYou can clone the repository at the command line if you wish by navigating to your local GitHub directory and running git clone TheURLYouCopied\nNow your local repository has been created, it’s time to do your magic coding.\nCreate a new branch for your changes It is a good idea to create a branch for your amazing new feature This enables you to work on coding for that feature in isolation. It has the added advantage that if you mess it right royally up, you can just delete that branch and start again with a new one!\nTo create a branch in VS Code or Azure Data Studio you can click on the branch name at the bottom left.\nOr open the Command Palette and type Branch until you see Git: Create Branch\nYou will be prompted for a branch name\nI like to choose a name that relates to the code that I am writing like configurable_engine or removeerroringexample You can see the name of the branch in the bottom left so that you always know which branch you are working on.\nThe icon shows that the branch is only local and hasn’t been pushed (published) to the remote repository yet\nMake some changes and commit them with useful messages Now you can start writing your code for your awesome new feature, bug fix or maybe just documentation improvement. Keep your commits small and give them useful commit messages that explain why you have made the change as the diff tooling will be able to show what change you have made\nWrite your code or change the documentation, save the file and in Visual Studio Code or Azure Data Studio you will see that the source control icon has a number on it\nClicking on the icon will show the files that have changes ready\nYou can write your commit message in the box and click CTRL + ENTER to commit your changes with a message\nIf you want to do this at the command line, you can use git status to see which files have changes\nYou will need to git add .or git add .\\pathtofile to stage your changes ready for committing and then git commit -m 'Commit Message' to commit them\nNotice that I did exactly what I just said not to do! A better commit message would have been So that people can find the guide to forking and creating a PR\nPush the changes to your repository You only have the changes that you have made in your local repository on your computer. Now you need to push those changes to GitHub your remote repository. You can click on the publish icon\nYou will get a pop-up asking you if you wish to stage your changes. I click Yes and never Always so that I can use this prompt as a sanity check that I am doing the right thing\nAt the command line you can push the branch, if you do that, you will have to tell git where the branch needs to go. If you just type git push it will helpfully tell you\nfatal: The current branch AwesomeNewFeature has no upstream branch.\rTo push the current branch and set the remote as upstream, use\rgit push --set-upstream origin AwesomeNewFeature\rSo you will need to use that command\nYou can see in the bottom left that the icon has changed\nand if you read the output of the git push command you will see what the next step is also.\nCreate a Pull Request from your repository back to the original one You can CTRL click the link in the git push output if you have pushed from the command line or if you visit either you repository or the original repository in your browser you will see that there is a Compare and Pull Request button\nYou click that and let GitHub do its magic\nand it will create a Pull Request for you ready for you to fill in the required information, ask for reviewers and other options. Once you have done that you can click Create pull request and wait for the project maintainer to review it and (hopefully) accept it into their project\nYou can find the Pull Request that I created here https://github.com/dataplat/dbachecks/pull/720 and see how the rest of this blog post was created.\nIf you make more changes to the code in the same branch in your local repository and push them, they will automatically be added to this Pull Request whilst it is open. You can do this if the maintainer or reviewer asks for changes.\nShane has asked for a change\nSo I can go to my local repository in Azure Data Studio and make the requested change and save the file. If I look in the source control in Azure Data Studio I can again see there is a change waiting to be committed and if I click on the name of the file I can open the diff tool to see what the change was\nOnce I am happy with my change I can commit it again in the same way as before either in the editor or at the command line. The icon at the bottom will change to show that I have one commit in my local repository waiting to be pushed\nTo do the same thing at the command line I can type git status and see the same thing.\nI can then push my change to my remote repository either in the GUI or by using git push\nand it will automatically be added to the Pull Request as you can see\nNow that the required changes for the review have been made, the review has been approved by Shane and the pull request is now ready to be merged. (You can also see that dbachecks runs some checks against the code when a Pull Request is made)\nMany, many thanks to Shane b | t who helped with the writing of this post even whilst on a “no tech” holiday.\nGo Ahead – Contribute to an Open Source Project Hopefully you can now see how easy it is to create a fork of a GitHub repository, clone it to your own machine and contribute. There are many open source projects that you can contribute to.\nYou can use this process to contribute to the Microsoft Docs for example by clicking on the edit button on any page.\nYou can contribute other open source projects like\nPowerShell by Microsoft tigertoolbox by Microsoft Tiger Team dbatools dbachecks ADSNotebook PSDatabaseClone OpenQueryStore by William Durkin and Enrico van de Laar sqlwatch by Marcin Gminski SQLCop by Redgate sp_whoisactive by Adam Machanic sql-server-maintenance-solution by Ola Hallengren SQL-Server-First-Responder-Kit by Brent Ozar Unlimited Pester ReportingServicesTools or go and find the the ones that you use and can help with.\n","date":"2019-11-29T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2019/11/CreatePR.png","permalink":"https://dbachecks.io/blog/how-to-fork-a-github-repository-and-contribute-to-an-open-source-project/","title":"How to fork a GitHub repository and contribute to an open source project"},{"content":"My wonderful friend André Kamman wrote a fantastic blog post this week SQL Server Container Instances via Cloudshell about how he uses containers in Azure to test code against different versions of SQL Server.\nIt reminded me that I do something very similar to test dbachecks code changes. I thought this might make a good blog post. I will talk through how I do this locally as I merge a PR from another great friend Cláudio Silva who has added agent job history checks.\nGitHub PR VS Code Extension I use the GitHub Pull Requests extension for VS Code to work with pull requests for dbachecks. This enables me to see all of the information about the Pull Request, merge it, review it, comment on it all from VS Code\nI can also see which files have been changed and which changes have been made\nOnce I am ready to test the pull request I perform a checkout using the extension\nThis will update all of the files in my local repository with all of the changes in this pull request\nYou can see at the bottom left that the branch changes from development to the name of the PR.\nRunning The Unit Tests The first thing that I do is to run the Unit Tests for the module. These will test that the code is following all of the guidelines that we require and that the tests are formatted in the correct way for the Power Bi to parse. I have blogged about this here and here and we use this Pester in our CI process in Azure DevOps which I described here.\nI navigate to the root of the dbachecks repository on my local machine and run\n$testresults = Invoke-Pester .\\tests -ExcludeTag Integration -Show Fails -PassThru\rand after about a minute\nThank you Cláudio, the code has passed the tests 😉\nRunning Some Integration Tests The difference between Unit tests and Integration tests in a nutshell is that the Unit tests are testing that the code is doing what is expected without any other external influences whilst the Integration tests are checking that the code is doing what is expected when running on an actual environment. In this scenario we know that the code is doing what is expected but we want to check what it does when it runs against a SQL Server and even when it runs against multiple SQL Servers of different versions.\nMultiple Versions of SQL Server As I have described before my friend and former colleague Andrew Pruski b | t has many resources for running SQL in containers. This means that I can quickly and easily create fresh uncontaminated instances of SQL 2012, 2014, 2016 and 2017 really quickly.\nI can create 4 instances of different versions of SQL in (a tad over) 1 minute. How about you?\nImagine how long it would take to run the installers for 4 versions of SQL and the pain you would have trying to uninstall them and make sure everything is ‘clean’. Even images that have been sysprep’d won’t be done in 1 minute.\nDocker Compose Up ? So what is this magic command that has enabled me to do this? docker compose uses a YAML file to define multi-container applications. This means that with a file called docker-compose.yml like this\nversion: '3.7'\rservices:\rsql2012:\rimage: dbafromthecold/sqlserver2012dev:sp4\rports:\r- \u0026quot;15589:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rsql2014:\rimage: dbafromthecold/sqlserver2014dev:sp2\rports:\r- \u0026quot;15588:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rsql2016:\rimage: dbafromthecold/sqlserver2016dev:sp2\rports:\r- \u0026quot;15587:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rsql2017:\rimage: microsoft/ mssql-server-windows-developer:2017-latest\rports:\r- \u0026quot;15586:1433\u0026quot;\renvironment:\rSA_PASSWORD: \u0026quot;Password0!\u0026quot;\rACCEPT_EULA: \u0026quot;Y\u0026quot;\rand in that directory just run\ndocker-compose up -d\rand 4 SQL containers are available to you. You can interact with them via SSMS if you wish with localhost comma PORTNUMBER. The port numbers in the above file are 15586, 15587,15588 and 15589\n](https://blog.robsewell.com/assets/uploads/2019/01/containers.png?ssl=1)\nNow it must be noted, as I describe here that first I pulled the images to my laptop. The first time you run docker compose will take significantly longer if you haven’t pulled the images already (pulling the images will take quite a while depending on your broadband speed)\nCredential The next thing is to save a credential to make it easier to automate.I use the method described by my PowerShell friend Jaap Brasser here.\nEDIT (September or is it March? 2020) - Nowadays I use the Secret Management Module\nI run this code\n$CredentialPath = 'C:\\MSSQL\\BACKUP\\KEEP\\sacred.xml'\rGet-Credential | Export-Clixml -Path $CredentialPath\rand then I can create a credential object using\n$cred = Import-Clixml $CredentialPath\rCheck The Connections I ensure a clean session by removing the dbatools and dbachecks modules and then import the local version of dbachecks and set some variables\n$dbacheckslocalpath = 'GIT:\\dbachecks\\'\rRemove-Module dbatools, dbachecks -ErrorAction SilentlyContinue\rImport-Module $dbacheckslocalpath\\dbachecks.psd1\r$cred = Import-Clixml $CredentialPath\r$containers = 'localhost,15589', 'localhost,15588', 'localhost, 15587', 'localhost,15586'\rNow I can start to run my Integration tests. First reset the dbachecks configuration and set some configuration values\n# run the checks against these instances\r$null = Set-DbcConfig -Name app.sqlinstance $containers\r# We are using SQL authentication\r$null = Set-DbcConfig -Name policy.connection.authscheme -Value SQL\r# sometimes its a bit slower than the default value\r$null = Set-DbcConfig -Name policy.network.latencymaxms -Value 100 # because the containers run a bit slow!\rThen I will run the dbachecks connectivity checks and save the results to a variable without showing any output\n$ConnectivityTests = Invoke-DbcCheck -SqlCredential $cred -Check Connectivity -Show None -PassThru\rI can then use Pester to check that dbachecks has worked as expected by testing if the failedcount property returned is 0.\nDescribe \u0026quot;Testing the checks are running as expected\u0026quot; -Tag Integration {\rContext \u0026quot;Connectivity Checks\u0026quot; {\rIt \u0026quot;All Tests should pass\u0026quot; {\r$ConnectivityTests.FailedCount | Should -Be 0 -Because \u0026quot;We expect all of the checks to run and pass with default settings\u0026quot;\r}\r}\r}\rWhat is the Unit Test for this PR? Next I think about what we need to be testing for the this PR. The Unit tests will help us.\nChoose some Integration Tests This check is checking the Agent job history settings and the unit tests are\nIt “Passes Check Correctly with Maximum History Rows disabled (-1)”\nIt “Fails Check Correctly with Maximum History Rows disabled (-1) but configured value is 1000”\nIt “Passes Check Correctly with Maximum History Rows being 10000”\nIt “Fails Check Correctly with Maximum History Rows being less than 10000”\nIt “Passes Check Correctly with Maximum History Rows per job being 100”\nIt “Fails Check Correctly with Maximum History Rows per job being less than 100”\nSo we will check the same things on real actual SQL Servers. First though we need to start the SQL Server Agent as it is not started by default. We can do this as follows\ndocker exec -ti integration_sql2012_1 powershell start-service SQLSERVERAGENT\rdocker exec -ti integration_sql2014_1 powershell start-service SQLSERVERAGENT\rdocker exec -ti integration_sql2016_1 powershell start-service SQLSERVERAGENT\rdocker exec -ti integration_sql2017_1 powershell start-service SQLSERVERAGENT\rUnfortunately, the agent service wont start in the SQL 2014 container so I cant run agent integration tests for that container but it’s better than no integration tests.\nThis is What We Will Test So we want to test if the check will pass with default settings. In general, dbachecks will pass for default instance, agent or database settings values by default.\nWe also want the check to fail if the configured value for dbachecks is set to default but the value has been set on the instance.\nWe want the check to pass if the configured value for the dbachecks configuration is set and the instance (agent, database) setting matches it.\nIf You Are Doing Something More Than Once …… Let’s automate that. We are going to be repeatedly running those three tests for each setting that we are running integration tests for. I have created 3 functions for this again checking that FailedCount or Passed Count is 0 depending on the test.\nfunction Invoke-DefaultCheck {\rIt \u0026quot;All Checks should pass with default for $Check\u0026quot; {\r$Tests = get-variable \u0026quot;$($Check)default\u0026quot; -ValueOnly\r$Tests.FailedCount | Should -Be 0 -Because \u0026quot;We expect all of the checks to run and pass with default setting (Yes we may set some values before but you get my drift)\u0026quot;\r}\r}\rfunction Invoke-ConfigCheck {\rIt \u0026quot;All Checks should fail when config changed for $Check\u0026quot; {\r$Tests = get-variable \u0026quot;$($Check)configchanged\u0026quot; -ValueOnly\r$Tests.PassedCount | Should -Be 0 -Because \u0026quot;We expect all of the checks to run and fail when we have changed the config values\u0026quot;\r}\r}\rfunction Invoke-ValueCheck {\rIt \u0026quot;All Checks should pass when setting changed for $Check\u0026quot; {\r$Tests = get-variable \u0026quot;$($Check) value changed\u0026quot; -ValueOnly\r$Tests.FailedCount | Should -Be 0 -Because \u0026quot;We expect all of the checks to run and pass when we have changed the settings to match the config values\u0026quot;\r}\r}\rNow I can use those functions inside a loop in my Integration Pester Test\n$TestingTheChecks = @('errorlogscount','jobhistory')\rForeach ($Check in $TestingTheChecks) {\rContext \u0026quot;$Check Checks\u0026quot; {\rInvoke-DefaultCheck\rInvoke-ConfigCheck\rInvoke-ValueCheck\r}\r}\rWrite Some Integration Tests So for this new test I have added a value to the TestingTheChecks array then I can test my checks. The default check I can check like this\n# run the checks against these instances (SQL2014 agent wont start :-( ))\r$null = Set-DbcConfig -Name app.sqlinstance $containers.Where {$_ -ne 'localhost,15588'}\r# by default all tests should pass on default instance settings\r$jobhistorydefault = Invoke-DbcCheck -SqlCredential $cred -Check JobHistory -Show None -PassThru\rNow I need to change the configurations so that they do not match the defaults and run the checks again\n#Change the configuration to test that the checks fail\r$null = Set-DbcConfig -Name agent.history. maximumjobhistoryrows -value 1000\r$null = Set-DbcConfig -Name agent.history.maximumhistoryrows -value 10000\r$jobhistoryconfigchanged = Invoke-DbcCheck -SqlCredential $cred -Check JobHistory -Show None -PassThru\rNext we have to change the instance settings so that they match the dbachecks configuration and run the checks and test that they all pass.\nWe will (of course) use dbatools for this. First we need to find the command that we need\nFind-DbaCommand jobserver\rand then work out how to use it\nGet-Help Set-DbaAgentServer -Detailed\rThere is an example that does exactly what we want 🙂 So we can run this.\n$setDbaAgentServerSplat = @{\rMaximumJobHistoryRows = 1000\rMaximumHistoryRows = 10000\rSqlInstance = $containers.Where{$_ -ne 'localhost,15588'}\rSqlCredential = $cred\r}\rSet-DbaAgentServer @setDbaAgentServerSplat\r$jobhistoryvaluechanged = Invoke-DbcCheck -SqlCredential $cred -Check JobHistory -Show None -PassThru\rRun the Integration Tests And then we will check that all of the checks are passing and failing as expected\nInvoke-Pester .\\DockerTests.ps1\rIntegration Test For Error Log Counts There is another integration test there for the error logs count. This works in the same way. Here is the code\n#region error Log Count - PR 583\r# default test\r$errorlogscountdefault = Invoke-DbcCheck -SqlCredential $cred -Check ErrorLogCount -Show None -PassThru\r# set a value and then it will fail\r$null = Set-DbcConfig -Name policy.errorlog.logcount -Value 10\r$errorlogscountconfigchanged = Invoke-DbcCheck -SqlCredential $cred -Check ErrorLogCount -Show None -PassThru\r# set the value and then it will pass\r$null = Set-DbaErrorLogConfig -SqlInstance $containers -SqlCredential $cred -LogCount 10\r$errorlogscountvaluechanged = Invoke-DbcCheck -SqlCredential $cred -Check ErrorLogCount -Show None -PassThru\r#endregion\rMerge the Changes So with all the tests passing I can merge the PR into the development branch and Azure DevOps will start a build. Ultimately, I would like to add the integration to the build as well following André‘s blog post but for now I used the GitHub Pull Request extension to merge the pull request into development which started a build and then merged that into master which signed the code and deployed it to the PowerShell gallery as you can see here and the result is\nhttps://www.powershellgallery.com/packages/dbachecks/1.1.164\n","date":"2019-01-19T00:00:00Z","permalink":"https://dbachecks.io/blog/using-docker-to-run-integration-tests-for-dbachecks/","title":"Using Docker to run Integration Tests for dbachecks"},{"content":"It’s been a few weeks since i have blogged as I have been busy with a lot of other things. One of which is preparing for my SQL Pass Summit pre-con which has lead to me improving the CI/CD for dbachecks by adding auto-creation of online documentation, which you can find at https://dbachecks.readthedocs.io or by running Get-Help with the -Online switch for any dbachecks command.\nGet-Help Invoke-DbcCheck -Online\nI will blog about how dbachecks uses Azure DevOps to do this another time\nPSPowerHour The PowerShell community members Michael T Lombardi and Warren Frame have created PSPowerHour. PSPowerHour is “like a virtual User Group, with a lightning-demo format, and room for non-PowerShell-specific content. Eight community members will give a demo each PowerHour.”\nChrissy blogged about the first one on the dbatools blog\nYou can watch the videos on the Youtube channel and keep an eye out for more online PSPowerHours via twitter or the GitHub page.\nWhile watching the first group of sessions Andrew Wickham demonstrated using dbatools with trace flags and I thought that needs to be added to dbachecks so I created an issue. Anyone can do this to file improvements as well as bugs for members of the team to code.\nTrace Flags The previous release of dbachecks brought 2 new checks for traceflags. One for traceflags expected to be running and one for traceflags not expected to be running.\nYou will need to have installed dbachecks from the PowerShell Gallery to do this. This can be done using\nInstall-Module -Name dbachecks\nOnce dbachecks is installed you can find the checks using\nGet-DBcCheck\nyou can filter using the pattern parameter\nGet-DBcCheck -Pattern traceflag\nThis will show you\nthe UniqueTag which will enable you to run only that check if you wish AllTags which shows which tags will include that check Config will show you which configuration items can be set for this check The trace flag checks require the app.sqlinstance configuration which is the list of SQL instances that the checks will run against. You can also specify the instances as a parameter for Invoke-DbCheck as well.\nThe configuration for the expected traceflags is policy.traceflags.expected By default it is set to null. You can see what configuration it has using\nGet-DBcConfig policy.traceflags.expected\nSo if you want to check that there are no trace flags running, then you can run\n$instance = \u0026lsquo;sql0\u0026rsquo; Set-DbcConfig -Name app.sqlinstance -Value $instance Invoke-DbcCheck -Check TraceFlagsExpected\nMaybe this instance is required to have trace flag 1117 enabled so that all files in a file group grow equally, you can set the trace flag you expect to be running using\nSet-DbcConfig -Name policy.traceflags.expected -Value 1117\nNow you when you run the check it fails\nInvoke-DbcCheck -Check TraceFlagsExpecte\nand gives you the error message\n[-] Expected Trace Flags 1117 exist on sql0 593ms Expected 1117 to be found in collection @(), because We expect that Trace Flag 1117 will be set on sql0, but it was not found.\nSo we have a failing test. We need to fix that. We can use dbatools\nEnable-DbaTraceFlag -SqlInstance $instance -TraceFlag 1117\nThis time when we run the check\nInvoke-DbcCheck -Check TraceFlagsExpected\nit passes\nIf you just need to see what trace flags are enabled you can use\nGet-DbaTraceFlag -SqlInstance $instance\nReset the configuration for the expected trace flag to an empty array and then set the configuration for traceflags we do not expect to be running to 1117\n1 2 Set-DbcConfig -Name policy.traceflags.expected -Value @() Set-DbcConfig -Name policy.traceflags.notexpected -Value 1117 and then run the trace flags not expected to be running check with\nInvoke-DbcCheck -Check TraceFlagsNotExpected\nIt will fail as 1117 is still running\nand give the message\n[-] Expected Trace Flags 1117 to not exist on sql0 321ms Expected 1117 to not be found in collection 1117, because We expect that Trace Flag 1117 will not be set on sql0, but it was found.\nSo to resolve this failing check we need to disable the trace flag and we can do that with dbatools using\nDisable-DbaTraceFlag -SqlInstance $instance -TraceFlag 1117\nand now when we run the check\nInvoke-DbcCheck -Check TraceFlagsNotExpected\nit passes\nThe checks also work with multiple traceflags so you can set multiple values for trace flags that are not expexted to be running\nSet-DbcConfig -Name policy.traceflags.notexpected -Value 1117, 1118\nand as we saw earlier, you can run both trace flag checks using\nInvoke-DbcCheck -Check TraceFlag\nYou can use this or any of the 95 available checks to validate that your SQL instances, singular or your whole estate are as you expect them to be.\n","date":"2018-09-29T00:00:00Z","permalink":"https://dbachecks.io/blog/checking-trace-flags-with-dbachecks-online-docs-and-pspowerhour/","title":"Checking Trace Flags with dbachecks, online docs and PSPowerHour"},{"content":"In dbachecks we enable people to see what checks are available by running Get-DbcCheck. This gives a number of properties including the ‘type’ of check. This refers to the configuration item or parameter that is required to have a value for this check to run.\nFor example – Any check to do with SQL Agent is of type Sqlinstance because it requires an instance to be specified but a check for SPN is of type ComputerName because it requires a computer name to run.\nAutomation for the win Because I believe in automation I do not want to have to hard code these values anywhere but create them when the module is imported so we use a json file to feed Get-DbcCheck and populate the Json file when we import the module. This is done using the method that I described here and means that whenever a new check is added it is automatically available in Get-DbcCheck without any extra work.\nWe use code like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## Parse the file with AST $CheckFileAST = [Management.Automation.Language.Parser]::ParseInput($check, [ref]$null, [ref]$null) ## Old code we can use the describes $Describes = $CheckFileAST.FindAll([Func[Management.Automation.Language.Ast, bool]] { param ($ast) $ast.CommandElements -and $ast.CommandElements[0].Value -eq \u0026#39;describe\u0026#39; }, $true) @($describes).ForEach{ $groups += $filename $Describe = $_.CommandElements.Where{$PSItem.StaticType.name -eq \u0026#39;string\u0026#39;}[1] $title = $Describe.Value $Tags = $PSItem.CommandElements.Where{$PSItem.StaticType.name -eq \u0026#39;Object[]\u0026#39; -and $psitem.Value -eq $null}.Extent.Text.ToString().Replace(\u0026#39;, $filename\u0026#39;, \u0026#39;\u0026#39;) # CHoose the type if ($Describe.Parent -match \u0026#34;Get-Instance\u0026#34;) { $type = \u0026#34;Sqlinstance\u0026#34; } elseif ($Describe.Parent -match \u0026#34;Get-ComputerName\u0026#34; -or $Describe.Parent -match \u0026#34;AllServerInfo\u0026#34;) { $type = \u0026#34;ComputerName\u0026#34; } elseif ($Describe.Parent -match \u0026#34;Get-ClusterObject\u0026#34;) { $Type = \u0026#34;ClusteNode\u0026#34; } First we parse the code with the AST and store that in the CheckFileAST variable, then we use the FindAll method to find any command elements that match “Describe” which conveniently gets our describes and then we can simply match the Parent object which holds some code to each function that we use to get our values to be passed to the tests Get-ComputerName, Get-Instance, Get-ClusterObject and set the type appropriately.\nwhich when run against a check like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Describe \u0026#34;Backup Path Access\u0026#34; -Tags BackupPathAccess, Storage, DISA, $filename { @(Get-Instance).ForEach{ if ($NotContactable -contains $psitem) { Context \u0026#34;Testing Backup Path Access on $psitem\u0026#34; { It \u0026#34;Can\u0026#39;t Connect to $Psitem\u0026#34; { $false| Should -BeTrue -Because \u0026#34;The instance should be available to be connected to!\u0026#34; } } } else { Context \u0026#34;Testing Backup Path Access on $psitem\u0026#34; { $backuppath = Get-DbcConfigValue policy.storage.backuppath if (-not$backuppath) { $backuppath = (Get-DbaDefaultPath-SqlInstance $psitem).Backup } It \u0026#34;can access backup path ($backuppath) on $psitem\u0026#34; { Test-DbaSqlPath-SqlInstance $psitem -Path $backuppath| Should -BeTrue -Because \u0026#39;The SQL Service account needs to have access to the backup path to backup your databases\u0026#39; } } } } } will find the describe block and get the title “Backup Path Access” and the tags BackupPathAccess, Storage, DISA, $filename and then find the Get-Instance and set the type to SqlInstance\nUntil Rob breaks it! This has worked wonderfully well for 6 months or so of the life of dbachecks but this week I broke it!\nThe problem was the performance of the code. It is taking a long time to run the tests and I am looking at ways to improve this. I was looking at the Server.Tests file because I thought why not start with one of the smaller files.\nIt runs the following checks\nServer Power Plan Configuration SPNs Disk Space Ping Computer CPUPrioritisation Disk Allocation Unit Instance Connection and it was looping through the computer names for each check like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Describe \u0026#34;Server Power Plan Configuration\u0026#34; -Tags PowerPlan, $filename { @(Get-ComputerName).ForEach{ } } Describe \u0026#34;Instance Connection\u0026#34; -Tags InstanceConnection, Connectivity, $filename { @(Get-Instance).ForEach{ } } Describe \u0026#34;SPNs\u0026#34; -Tags SPN, $filename { @(Get-ComputerName).ForEach{ } } Describe \u0026#34;Disk Space\u0026#34; -Tags DiskCapacity, Storage, DISA, $filename { @(Get-ComputerName).ForEach{ } } Describe \u0026#34;Ping Computer\u0026#34; -Tags PingComputer, $filename { @(Get-ComputerName).ForEach{ } } Describe \u0026#34;CPUPrioritisation\u0026#34; -Tags CPUPrioritisation, $filename { @(Get-ComputerName).ForEach{ } } Describe \u0026#34;Disk Allocation Unit\u0026#34; -Tags DiskAllocationUnit, $filename { @(Get-ComputerName).ForEach{ } } I altered it to have only one loop for the computer names like so\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @(Get-ComputerName).ForEach{ Describe \u0026#34;Server Power Plan Configuration\u0026#34; -Tags PowerPlan, $filename { } Describe \u0026#34;SPNs\u0026#34; -Tags SPN, $filename { } Describe \u0026#34;Disk Space\u0026#34; -Tags DiskCapacity, Storage, DISA, $filename { } Describe \u0026#34;Ping Computer\u0026#34; -Tags PingComputer, $filename { } Describe \u0026#34;CPUPrioritisation\u0026#34; -Tags CPUPrioritisation, $filename { } Describe \u0026#34;Disk Allocation Unit\u0026#34; -Tags DiskAllocationUnit, $filename { } } Describe \u0026#34;Instance Connection\u0026#34; -Tags InstanceConnection, Connectivity, $filename { @(Get-Instance).ForEach{ } } and immediately in testing my checks for the Server Tag decreased in time by about 60% 🙂\nI was very happy.\nThen I added it to the dbachecks module on my machine, loaded the module and realised that my Json file for Get-DbcCheck was no longer being populated for the type because this line\n1 elseif ($Describe.Parent-match\u0026#34;Get-ComputerName\u0026#34;-or$Describe.Parent-match\u0026#34;AllServerInfo\u0026#34;) was no longer true.\nAST for other things So I googled Management.Automation.Language.Ast the first result lead me to docs.microsoft There are a number of different language elements available there and I found InvokeMemberExpressionAst which will let me find any methods that have been invoked, so now I can find the loops with\n1 2 3 4 $ComputerNameForEach = $CheckFileAST.FindAll([Func[Management.Automation.Language.Ast, bool]] { param ($ast) $ast -is [System.Management.Automation.Language.InvokeMemberExpressionAst] }, $true) When I examined the object returned I could see that I could further limit the result to get only the method for Get-ComputerName and then if I choose the Extent I can get the code of that loop\n1 2 3 4 5 ## New code uses a Computer Name loop to speed up execution so need to find that as well $ComputerNameForEach=$CheckFileAST.FindAll([Func[Management.Automation.Language.Ast,bool]] { param ($ast) $ast-is [System.Management.Automation.Language.InvokeMemberExpressionAst] -and$ast.expression.Subexpression.Extent.Text-eq\u0026#39;Get-ComputerName\u0026#39; }, $true).Extent and now I can match the Tags to the type again :-)\n1 2 3 if ($ComputerNameForEach-match$title) { $type=\u0026#34;ComputerName\u0026#34; } and now Get-DbcCheck is returning the right results and the checks are a little faster\nYou can find dbachecks on the PowerShell Gallery or install it using\nInstall-Module dbachecks -Scope CurrentUser\n","date":"2018-08-16T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/08/server.png","permalink":"https://dbachecks.io/blog/using-the-powershell-ast-to-find-a-foreach-method/","title":"Using the PowerShell AST to find a ForEach Method"},{"content":"I like to write Pester checks to make sure that all is as expected! This is just a quick post as much to help me remember this script 🙂\nThis is a quick Pester test I wrote to ensure that some SQL Scripts in a directory would parse so there was some guarantee that they were valid T-SQL. It uses the SQLParser.dll and because it was using a build server without SQL Server I have to load the required DLLs from the dbatools module (Thank you dbatools 🙂 )\nIt simply runs through all of the .sql files and runs the parser against them and checks the errors. In the case of failures it will output where it failed in the error message in the failed Pester result as well.\nYou will need dbatools module installed on the instance and at least version 4 of the Pester module as well\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Describe \u0026#34;Testing SQL\u0026#34; { Context \u0026#34;Running Parser\u0026#34; { ## Load assembly $Parserdll = (Get-ChildItem \u0026#39;C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules\\\\dbatools\u0026#39; -Include Microsoft.SqlServer.Management.SqlParser.dll -Recurse)\\[0\\].FullName \\[System.Reflection.Assembly\\]::LoadFile($Parserdll) | Out-Null $TraceDll = (Get-ChildItem \u0026#39;C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules\\\\dbatools\u0026#39; -Include Microsoft.SqlServer.Diagnostics.Strace.dll -Recurse)\\[0\\].FullName \\[System.Reflection.Assembly\\]::LoadFile($TraceDll) | Out-Null $ParseOptions = New-Object Microsoft.SqlServer.Management.SqlParser.Parser.ParseOptions $ParseOptions.BatchSeparator = \u0026#39;GO\u0026#39; $files = Get-ChildItem -Path $Env:Directory -Include *.sql -Recurse ## This variable is set as a Build Process Variable or put your path here $files.ForEach{ It \u0026#34;$($Psitem.FullName) Should Parse SQL correctly\u0026#34; { $filename = $Psitem.FullName $sql = Get-Content -LiteralPath \u0026#34;$fileName\u0026#34; $Script = \\[Microsoft.SqlServer.Management.SqlParser.Parser.Parser\\]::Parse($SQL, $ParseOptions) $Script.Errors | Should -BeNullOrEmpty } } } } ","date":"2018-07-25T00:00:00Z","permalink":"https://dbachecks.io/blog/a-powershell-pester-check-for-parsing-sql-scripts/","title":"A PowerShell Pester Check for parsing SQL scripts"},{"content":"in my last post I showed how you can save the results of dbachecks to a database and created a PowerBi report. Inspired by Frank Henninger in the #dbachecks slack channel and Shawn Melton who explained the difficulties with red/green colour blind I then created this one 🙂\nYou can find it in my GitHub and have a play with it below\n","date":"2018-05-28T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/05/dark-mode.png","permalink":"https://dbachecks.io/blog/dbachecks-dark-mode-historical-validation-powerbi/","title":"dbachecks – Dark Mode Historical Validation PowerBi"},{"content":"I gave a presentation at SQL Day in Poland last week on dbachecks and one of the questions I got asked was will you write a command to put the results of the checks into a database for historical reporting.\nThe answer is no and here is the reasoning. The capability is already there. Most good PowerShell commands will only return an object and the beauty of an object is that you can do anything you like with it. Your only limit is your imagination 🙂 I have written about this before here. The other reason is that it would be very difficult to write something that was easily configurable for the different requirements that people will require. But here is one way of doing it.\nCreate a configuration and save it Let’s define a configuration and call it production. This is something that I do all of the time so that I can easily run a set of checks with the configuration that I want.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # The computername we will be testing Set-DbcConfig -Name app.computername -Value $sql0,$SQl1 # The Instances we want to test Set-DbcConfig -Name app.sqlinstance -Value $sql0,$SQl1 # The database owner we expect Set-DbcConfig -Name policy.validdbowner.name -Value \u0026#39;THEBEARD\\\\EnterpriseAdmin\u0026#39; # the database owner we do NOT expect Set-DbcConfig -Name policy.invaliddbowner.name -Value \u0026#39;sa\u0026#39; # Should backups be compressed by default? Set-DbcConfig -Name policy.backup.defaultbackupcompression -Value $true # Do we allow DAC connections? Set-DbcConfig -Name policy.dacallowed -Value $true # What recovery model should we have? Set-DbcConfig -Name policy.recoverymodel.type -value FULL # What should ourt database growth type be? Set-DbcConfig -Name policy.database.filegrowthtype -Value kb # What authentication scheme are we expecting? Set-DbcConfig -Name policy.connection.authscheme -Value \u0026#39;KERBEROS\u0026#39; # Which Agent Operator should be defined? Set-DbcConfig -Name agent.dbaoperatorname -Value \u0026#39;The DBA Team\u0026#39; # Which Agent Operator email should be defined? Set-DbcConfig -Name agent.dbaoperatoremail -Value \u0026#39;TheDBATeam@TheBeard.Local\u0026#39; # Which failsafe operator shoudl be defined? Set-DbcConfig -Name agent.failsafeoperator -Value \u0026#39;The DBA Team\u0026#39; ## Set the database mail profile name Set-DbcConfig -Name agent.databasemailprofile -Value \u0026#39;DbaTeam\u0026#39; # Where is the whoisactive stored procedure? Set-DbcConfig -Name policy.whoisactive.database -Value master # What is the maximum time since I took a Full backup? Set-DbcConfig -Name policy.backup.fullmaxdays -Value 7 # What is the maximum time since I took a DIFF backup (in hours) ? Set-DbcConfig -Name policy.backup.diffmaxhours -Value 26 # What is the maximum time since I took a log backup (in minutes)? Set-DbcConfig -Name policy.backup.logmaxminutes -Value 30 # What is my domain name? Set-DbcConfig -Name domain.name -Value \u0026#39;TheBeard.Local\u0026#39; # Where is my Ola database? Set-DbcConfig -Name policy.ola.database -Value master # Which database should not be checked for recovery model Set-DbcConfig -Name policy.recoverymodel.excludedb -Value \u0026#39;master\u0026#39;,\u0026#39;msdb\u0026#39;,\u0026#39;tempdb\u0026#39; # Should I skip the check for temp files on c? Set-DbcConfig -Name skip.tempdbfilesonc -Value $true # Should I skip the check for temp files count? Set-DbcConfig -Name skip.tempdbfilecount -Value $true # Which Checks should be excluded? Set-DbcConfig -Name command.invokedbccheck.excludecheck -Value LogShipping,ExtendedEvent, PseudoSimple,SPN, TestLastBackupVerifyOnly,IdentityUsage,SaRenamed # How many months before a build is unsupported do I want to fail the test? Set-DbcConfig -Name policy.build.warningwindow -Value 6 ## I need to set the app.cluster configuration to one of the nodes for the HADR check ## and I need to set the domain.name value Set-DbcConfig -Name app.cluster -Value $SQL0 Set-DbcConfig -Name domain.name -Value \u0026#39;TheBeard.Local\u0026#39; ## I also skip the ping check for the listener as we are in Azure Set-DbcConfig -Name skip.hadr.listener.pingcheck -Value $true Now I can export that configuration to a json file and store on a file share or in source control using the code below. This makes it easy to embed the checks into an automation solution\nExport-DbcConfig -Path Git:\\\\Production.Json\nand then I can use it with\n1 2 Import-DbcConfig -Path Git:\\\\Production.Json Invoke-DbcCheck I would use one of the Show parameter values here if I was running it at the command line, probably fails to make reading the information easier\nAdd results to a database This only gets us the test results on the screen, so if we want to save them to a database we have to use the PassThru parameter for Invoke-DbcCheck. I will run the checks again, save them to a variable\n$Testresults = Invoke-DbcCheck -PassThru -Show Fails\nThen I can use the dbatools Write-DbaDatatable command to write the results to a table in a database. I need to do this twice, once for the summary and once for the test results\n1 2 $Testresults | Write-DbaDataTable -SqlInstance $sql0 -Database tempdb -Table Prod_dbachecks_summary -AutoCreateTable $Testresults.TestResult | Write-DbaDataTable -SqlInstance $sql0 -Database tempdb -Table Prod_dbachecks_detail -AutoCreateTable and I get two tables one for the summary\nand one for the details\nThis works absolutely fine and I could continue to add test results in this fashion but it has no date property so it is not so useful for reporting.\nCreate tables and triggers This is one way of doing it. I am not sure it is the best way but it works! I always look forward to how people take ideas and move them forward so if you have a better/different solution please blog about it and reference it in the comments below\nFirst I created a staging table for the summary results\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 CREATE TABLE [dbachecks].[Prod_dbachecks_summary_stage]( [TagFilter] [nvarchar](max) NULL, [ExcludeTagFilter] [nvarchar](max) NULL, [TestNameFilter] [nvarchar](max) NULL, [TotalCount] [int] NULL, [PassedCount] [int] NULL, [FailedCount] [int] NULL, [SkippedCount] [int] NULL, [PendingCount] [int] NULL, [InconclusiveCount] [int] NULL, [Time] [bigint] NULL, [TestResult] [nvarchar](max) NULL ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO and a destination table with a primary key and a date column which defaults to todays date\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 CREATE TABLE [dbachecks].[Prod_dbachecks_summary]( [SummaryID] [int] IDENTITY(1,1) NOT NULL, [TestDate] [date] NOT NULL, [TagFilter] [nvarchar](max) NULL, [ExcludeTagFilter] [nvarchar](max) NULL, [TestNameFilter] [nvarchar](max) NULL, [TotalCount] [int] NULL, [PassedCount] [int] NULL, [FailedCount] [int] NULL, [SkippedCount] [int] NULL, [PendingCount] [int] NULL, [InconclusiveCount] [int] NULL, [Time] [bigint] NULL, [TestResult] [nvarchar](max) NULL, CONSTRAINT [PK_Prod_dbachecks_summary] PRIMARY KEY CLUSTERED ( [SummaryID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO ALTER TABLE [dbachecks].[Prod_dbachecks_summary] ADD CONSTRAINT [DF_Prod_dbachecks_summary_TestDate] DEFAULT (getdate()) FOR [TestDate] GO and added an INSERT trigger to the staging table\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 CREATE TRIGGER [dbachecks].[Load_Prod_Summary] ON [dbachecks].[Prod_dbachecks_summary_stage] AFTER INSERT AS BEGIN \\\\ SET NOCOUNT ON added to prevent extra result sets from \\\\ interfering with SELECT statements. SET NOCOUNT ON; INSERT INTO [dbachecks].[Prod_dbachecks_summary] ([TagFilter], [ExcludeTagFilter], [TestNameFilter], [TotalCount], [PassedCount], [FailedCount], [SkippedCount], [PendingCount], [InconclusiveCount], [Time], [TestResult]) SELECT [TagFilter], [ExcludeTagFilter], [TestNameFilter], [TotalCount], [PassedCount], [FailedCount], [SkippedCount], [PendingCount], [InconclusiveCount], [Time], [TestResult] FROM [dbachecks].[Prod_dbachecks_summary_stage] END GO ALTER TABLE [dbachecks].[Prod_dbachecks_summary_stage] ENABLE TRIGGER [Load_Prod_Summary] GO and for the details I do the same thing. A details table\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 CREATE TABLE [dbachecks].[Prod_dbachecks_detail]( [DetailID] [int] IDENTITY(1,1) NOT NULL, [SummaryID] [int] NOT NULL, [ErrorRecord] [nvarchar](max) NULL, [ParameterizedSuiteName] [nvarchar](max) NULL, [Describe] [nvarchar](max) NULL, [Parameters] [nvarchar](max) NULL, [Passed] [bit] NULL, [Show] [nvarchar](max) NULL, [FailureMessage] [nvarchar](max) NULL, [Time] [bigint] NULL, [Name] [nvarchar](max) NULL, [Result] [nvarchar](max) NULL, [Context] [nvarchar](max) NULL, [StackTrace] [nvarchar](max) NULL, CONSTRAINT [PK_Prod_dbachecks_detail] PRIMARY KEY CLUSTERED ( [DetailID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO ALTER TABLE [dbachecks].[Prod_dbachecks_detail] WITH CHECK ADD CONSTRAINT [FK_Prod_dbachecks_detail_Prod_dbachecks_summary] FOREIGN KEY([SummaryID]) REFERENCES [dbachecks].[Prod_dbachecks_summary] ([SummaryID]) GO ALTER TABLE [dbachecks].[Prod_dbachecks_detail] CHECK CONSTRAINT [FK_Prod_dbachecks_detail_Prod_dbachecks_summary] GO A stage table\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE TABLE [dbachecks].[Prod_dbachecks_detail_stage]( [ErrorRecord] [nvarchar](max) NULL, [ParameterizedSuiteName] [nvarchar](max) NULL, [Describe] [nvarchar](max) NULL, [Parameters] [nvarchar](max) NULL, [Passed] [bit] NULL, [Show] [nvarchar](max) NULL, [FailureMessage] [nvarchar](max) NULL, [Time] [bigint] NULL, [Name] [nvarchar](max) NULL, [Result] [nvarchar](max) NULL, [Context] [nvarchar](max) NULL, [StackTrace] [nvarchar](max) NULL ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO with a trigger\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 CREATE TRIGGER [dbachecks].[Load_Prod_Detail] ON [dbachecks].[Prod_dbachecks_detail_stage] AFTER INSERT AS BEGIN \\\\ SET NOCOUNT ON added to prevent extra result sets from \\\\ interfering with SELECT statements. SET NOCOUNT ON; INSERT INTO [dbachecks].[Prod_dbachecks_detail] ([SummaryID],[ErrorRecord], [ParameterizedSuiteName], [Describe], [Parameters], [Passed], [Show], [FailureMessage], [Time], [Name], [Result], [Context], [StackTrace]) SELECT (SELECT MAX(SummaryID) From [dbachecks].[Prod_dbachecks_summary]),[ErrorRecord], [ParameterizedSuiteName], [Describe], [Parameters], [Passed], [Show], [FailureMessage], [Time], [Name], [Result], [Context], [StackTrace] FROM [dbachecks].[Prod_dbachecks_detail_stage] END GO ALTER TABLE [dbachecks].[Prod_dbachecks_detail_stage] ENABLE TRIGGER [Load_Prod_Detail] GO Then I can use Write-DbaDatatable with a couple of extra parameters, FireTriggers to run the trigger, Truncate and Confirm:$false to avoid any confirmation because I want this to run without any interaction and I can get the results into the database.\n1 2 $Testresults | Write-DbaDataTable -SqlInstance $Instance -Database $Database -Schema dbachecks -Table Prod_dbachecks_summary_stage -FireTriggers -Truncate -Confirm:$False $Testresults.TestResult | Write-DbaDataTable -SqlInstance $Instance -Database $Database -Schema dbachecks -Table Prod_dbachecks_detail_stage -FireTriggers -Truncate -Confirm:$False Which means that I can now query some of this data and also create PowerBi reports for it.\nTo enable me to have results for the groups in dbachecks I have to do a little bit of extra manipulation. I can add all of the checks to the database using\n1 Get-DbcCheck | Write-DbaDataTable -SqlInstance $sql0 -Database ValidationResults -Schema dbachecks -Table Checks -Truncate -Confirm:$False -AutoCreateTable But because the Ola Hallengren Job names are configuration items I need to update the values for those checks which I can do as follows\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $query = \u0026#34; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.systemfull) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$SysFullJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.UserFull) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$UserFullJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.UserDiff) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$UserDiffJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.UserLog) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$UserLogJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.CommandLogCleanup) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$CommandLogJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.SystemIntegrity) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$SysIntegrityJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.UserIntegrity) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$UserIntegrityJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.UserIndex) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$UserIndexJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.OutputFileCleanup) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$OutputFileJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.DeleteBackupHistory) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$DeleteBackupJobName\u0026#39; UPDATE [dbachecks].[Checks] SET [Describe] = \u0026#39;Ola - \u0026#34; + (Get-DbcConfigValue -Name ola.jobname.PurgeBackupHistory) + \u0026#34;\u0026#39; WHERE [Describe] = \u0026#39;Ola - `$PurgeBackupJobName\u0026#39; \u0026#34; Invoke-DbaSqlQuery -SqlInstance $SQL0 -Database ValidationResults -Query $query You can get a sample Power Bi report in my Github which also has the code from this blog post\nThen you just need to open in PowerBi Desktop and\nClick Edit Queries\nClick Data Source Settings\nClick Change Source\nChange the Instance and Database names\nThen have an interactive report like this. Feel free to click around and see how it works. Use the arrows at the bottom right to go full-screen. NOTE – it filters by “today” so if I haven’t run the check and the import then click on one of the groups under “Today’s Checks by Group”\nThis enables me to filter the results and see what has happened in the past so I can filter by one instance\nor I can filter by a group of tests\nor even by a group of tests for an instance\nHopefully, this will give you some ideas of what you can do with your dbachecks results. You can find all of the code and the PowerBi in my GitHub\nHappy Validating!\n","date":"2018-05-23T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/05/08-filter-by-instance-and-insance.png","permalink":"https://dbachecks.io/blog/dbachecks-save-the-results-to-a-database-for-historical-reporting/","title":"dbachecks – Save the results to a database for historical reporting"},{"content":"With the latest release of dbachecks we have added a new check for testing that foreign keys and constraints are trusted thanks to Cláudio Silva b | t\nTo get the latest release you will need to run\nUpdate-Module dbachecks\rYou should do this regularly as we release new improvements frequently.\nWe have also added better descriptions for the checks which was suggested by the same person who inspired the previous improvement I blogged about here\nInstead of the description just being the name of the check it is now more of a, well, a description really 🙂\nThis has the added effect that it means that just running Get-DbcCheck in the command line will not fit all of the information on a normal screen\nYou can use the Format-Table command (or its alias ft at the command line) and select the properties to display using\nGet-DbcCheck | ft -Property UniqueTag, Description -Wrap\ror you can use Format-List (or its alias fl at the command line)\nGet-DbcCheck | fl\rOr you can use Out-GridView (or its alias ogv at the command line) (Incidentally, could you also thumbs up this issue on Github to get Out-GridView functionality in PowerShell 6)\nGet-DbcCheck | ogv\rHappy Validating !\n","date":"2018-05-19T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/05/04-get-dbacheck-ogv.png","permalink":"https://dbachecks.io/blog/dbachecks-improved-descriptions/","title":"dbachecks – Improved Descriptions"},{"content":"I love showing dbachecks to people. It’s really cool seeing how people will use it and listening to their experiences. I was showing it to a production DBA a month or so ago and he said\nHow Do I Know Which Checks There Are? OK you just need to run\nGet-DbcCheck\nand it will show you\nIt will show you the group, the type (does it need a computer name or an instance name), The description, the unique tag for running just that check and all the tags that will run that check\nOK he said, you talked about configurations\nHow Do I Know Which Configurations There Are? So to do that you just need to run\nGet-DbcConfig\nand it will show you\nYou can see the name, the current value and the description\nAh thats cool he said so\nHow Do I Know Which Configuration Is For Which Check? Well, you just…. , you know…… AHHHHHHH\nPing – light bulb moment!\nIt’s always really useful to give something you have built to people who have never seen it before and then listen to what they say. Their new eyes and different experiences or expectations will give you lots of insight\nNone of the amazing contributors to dbachecks had thought of this scenario so I decided to fix this. First I asked for an issue to be raised in GitHub because an issue can be an improvement or a suggestion not just a bug.\nThen I fixed it so that it would do what was required. Thank you Nick for this feedback and for helping to improve dbachecks\nI improved Get-DbcCheck so that now it shows the configuration item related to each check\nIt is easier to see (and sort or search) if you use Out-GridView\nGet-DbcCheck | Out-GridView\rSo now you can see which configuration can be set for each check!\nHappy Validating!\n","date":"2018-05-15T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/05/03-New-dbccheck.png","permalink":"https://dbachecks.io/blog/dbachecks-which-configuration-item-for-which-check/","title":"dbachecks – Which Configuration Item For Which Check ?"},{"content":"I am working on my dbatools and dbachecks presentations for SQL Saturday Finland, SQLDays, SQL Saturday Cork and SQLGrillen I want to show the two modules running against a number of SQL Versions so I have installed\n2 Domain Controllers 2 SQL 2017 instances on Windows 2016 with an Availability Group and WideWorldImporters database 1 Windows 2016 jump box with all the programmes I need 1 Windows 2016 with containers using a VSTS build and this set of ARM templates and scripts\nI wanted to create containers running SQL2017, SQL2016, SQL2014 and SQL2012 and restore versions of the AdventureWorks database onto each one.\nMove Docker Location I redirected my docker location from my C:\\ drive to my E:\\ drive so I didnt run out of space. I did this by creating a daemon.json file in C:\\ProgramData\\docker\\config and adding\n{\u0026quot;data-root\u0026quot;: \u0026quot;E:\\containers\u0026quot;}\nand restarting the docker service which created folders like this\nThen I ran\ndocker volume create SQLBackups\nto create a volume to hold the backups that I could mount on the containers\nAdventureWorks Backups I downloaded all the AdventureWorks backups from GitHub and copied them to E:\\containers\\volumes\\sqlbackups\\_data\nGet-ChildItem $Home\\Downloads\\AdventureWorks* | Copy-Item -Destination E:\\containers\\volumes\\sqlbackups\\_data\nGetting the Images To download the SQL 2017 image from the DockerHub I ran\ndocker pull microsoft/mssql-server-windows-developer:latest\nand waited for it to download and extract\nI also needed the images for other versions. My good friend Andrew Pruski b | t has versions available for us to use on his Docker Hub so it is just a case of running\n1 2 3 docker pull dbafromthecold/sqlserver2016dev:sp1 docker pull dbafromthecold/sqlserver2014dev:sp2 docker pull dbafromthecold/sqlserver2012dev:sp4 and waiting for those to download and extract (This can take a while!)\nCreate the containers Creating the containers is as easy as\ndocker run -d -p ExposedPort:InternalPort --name NAME -v VolumeName:LocalFolder -e sa\\_password=THEPASSWORD -e ACCEPT\\_EULA=Y IMAGENAME\nso all I needed to run to create 4 SQL containers one of each version was\n1 2 3 4 docker run -d -p 15789:1433 --name 2017 -v sqlbackups:C:\\SQLBackups -e sa\\_password=PruskiIsSQLContainerMan! -e ACCEPT\\_EULA=Y microsoft/mssql-server-windows-developer docker run -d -p 15788:1433 --name 2016 -v sqlbackups:C:\\SQLBackups -e sa\\_password=PruskiIsSQLContainerMan! -e ACCEPT\\_EULA=Y dbafromthecold/sqlserver2016dev:sp1 docker run -d -p 15787:1433 --name 2014 -v sqlbackups:C:\\SQLBackups -e sa\\_password=PruskiIsSQLContainerMan! -e ACCEPT\\_EULA=Y dbafromthecold/sqlserver2014dev:sp2 docker run -d -p 15786:1433 --name 2012 -v sqlbackups:C:\\SQLBackups -e sa\\_password=PruskiIsSQLContainerMan! -e ACCEPT\\_EULA=Y dbafromthecold/sqlserver2012dev:sp4 and just a shade over 12 seconds later I have 4 SQL instances ready for me 🙂\nStoring Credentials This is not something I would do in a Production environment but I save my credentials using this method that Jaap Brasser b | t shared here\nGet-Credential | Export-Clixml -Path $HOME\\Documents\\sa.cred\nwhich means that I can get the credentials in my PowerShell session (as long as it is the same user that created the file) using\n$cred = Import-Clixml $HOME\\Documents\\sa.cred\nRestoring the databases I restored all of the AdventureWorks databases that each instance will support onto each instance, so 2017 has all of them whilst 2012 only has the 2012 versions.\nFirst I needed to get the filenames of the backup files into a variable\n$filenames = (Get-ChildItem '\\bearddockerhost\\e$\\containers\\volumes\\sqlbackups\\_data').Name\nand the container connection strings, which are the hostname and the port number\n$containers = 'bearddockerhost,15789', 'bearddockerhost,15788', 'bearddockerhost,15787', 'bearddockerhost,15786'\nthen I can restore the databases using dbatools using a switch statement on the version which I get with the NameLevel property of Get-DbaSqlBuildReference\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 $cred = Import-Clixml $HOME\\Documents\\sa.cred $containers = \u0026#39;bearddockerhost,15789\u0026#39;, \u0026#39;bearddockerhost,15788\u0026#39;, \u0026#39;bearddockerhost,15787\u0026#39;, \u0026#39;bearddockerhost,15786\u0026#39; $filenames = (Get-ChildItem \u0026#39;\\bearddockerhost\\e$\\containers\\volumes\\sqlbackups\\_data\u0026#39;).Name $containers.ForEach{ $Container = $Psitem $NameLevel = (Get-DbaSqlBuildReference-SqlInstance $Container-SqlCredential $cred).NameLevel switch ($NameLevel) { 2017 { Restore-DbaDatabase-SqlInstance $Container-SqlCredential $cred-Path C:\\sqlbackups\\ -useDestinationDefaultDirectories -WithReplace |Out-Null Write-Verbose-Message \u0026#34;Restored Databases on 2017\u0026#34; } 2016 { $Files = $Filenames.Where{$PSitem -notlike \u0026#39;\\*2017\\*\u0026#39;}.ForEach{\u0026#39;C:\\sqlbackups\\\u0026#39; + $Psitem} Restore-DbaDatabase-SqlInstance $Container-SqlCredential $cred-Path $Files-useDestinationDefaultDirectories -WithReplace Write-Verbose-Message \u0026#34;Restored Databases on 2016\u0026#34; } 2014 { $Files = $Filenames.Where{$PSitem -notlike \u0026#39;\\*2017\\*\u0026#39; -and $Psitem -notlike \u0026#39;\\*2016\\*\u0026#39;}.ForEach{\u0026#39;C:\\sqlbackups\\\u0026#39; + $Psitem} Restore-DbaDatabase-SqlInstance $Container-SqlCredential $cred-Path $Files-useDestinationDefaultDirectories -WithReplace Write-Verbose-Message \u0026#34;Restored Databases on 2014\u0026#34; } 2012 { $Files = $Filenames.Where{$PSitem -like \u0026#39;\\*2012\\*\u0026#39;}.ForEach{\u0026#39;C:\\sqlbackups\\\u0026#39; + $Psitem} Restore-DbaDatabase-SqlInstance $Container-SqlCredential $cred-Path $Files-useDestinationDefaultDirectories -WithReplace Write-Verbose-Message \u0026#34;Restored Databases on 2012\u0026#34; } Default {} } } I need to create the file paths for each backup file by getting the correct backups and appending the names to C:\\SQLBackups which is where the volume is mounted inside the container\nAs Get-DbaDatabase gives the container ID as the Computer Name I have highlighted each container below\nThat is how easy it is to create a number of SQL containers of differing versions for your presentations or exploring needs\nHappy Automating!\n","date":"2018-05-10T00:00:00Z","permalink":"https://dbachecks.io/blog/creating-sql-server-containers-for-versions-2012-2017/","title":"Creating SQL Server Containers for versions 2012-2017"},{"content":"At the fabulous PowerShell Conference EU I presented about Continuous Delivery to the PowerShell Gallery with VSTS and explained how we use VSTS to enable CD for dbachecks. We even released a new version during the session 🙂\nSo how do we achieve this?\nWe have a few steps\nCreate a project and link to our GitHub Run unit uests with Pester to make sure that our code is doing what we expect. Update our module version and commit the change to GitHub Sign our code with a code signing certificate Publish to the PowerShell Gallery Create Project and link to GitHub First you need to create a VSTS project by going to https://www.visualstudio.com/ This is free for up to 5 users with 1 concurrent CI/CD queue limited to a maximum of 60 minutes run time which should be more than enough for your PowerShell module.\nClick on Get Started for free under Visual Studio Team Services and fill in the required information. Then on the front page click new project\nFill in the details and click create\nClick on builds and then new definition\nnext you need to link your project to your GitHub (or other source control providers) repository\nYou can either authorise with OAuth or you can provide a PAT token following the instructions here. Once that is complete choose your repo. Save the PAT as you will need it later in the process!\nand choose the branch that you want this build definition to run against.\nI chose to run the Unit Tests when a PR was merged into the development branch. I will then create another build definition for the master branch to sign the code and update module version. This enables us to push several PRs into the development branch and create a single release for the gallery.\nThen I start with an empty process\nand give it a suitable name\ni chose the hosted queue but you can download an agent to your build server if you need to do more or your integration tests require access to other resources not available on the hosted agent.\nRun Unit Tests with Pester We have a number of Unit tests in our tests folder in dbachecks so we want to run them to ensure that everything is as it should be and the new code will not break existing functionality (and for dbachecks the format of the PowerBi)\nYou can use the Pester Test Runner Build Task from the folk at Black Marble by clicking on the + sign next to Phase 1 and searching for Pester\nYou will need to click Get It Free to install it and then click add to add the task to your build definition. You can pretty much leave it as default if you wish and Pester will run all of the *.Tests.ps1 files that it finds in the directory where it downloads the GitHub repo which is referred to using the variable $(Build.SourcesDirectory). It will then output the results to a json file called Test-Pester.XML ready for publishing.\nHowever, as dbachecks has a number of dependent modules, this task was not suitable. I spoke with Chris Gardner b | t from Black Marble at the PowerShell Conference and he says that this can be resolved so look out for the update. Chris is a great guy and always willing to help, you can often find him in the PowerShell Slack channel answering questions and helping people\nBut as you can use PowerShell in VSTS tasks, this is not a problem although you need to write your PowerShell using try catch to make sure that your task fails when your PowerShell errors. This is the code I use to install the modules\n$ErrorActionPreference = \u0026lsquo;Stop\u0026rsquo;\n# Set location to module home path in artifacts directory try { Set-Location $(Build.SourcesDirectory) Get-ChildItem } catch { Write-Error \u0026ldquo;Failed to set location\u0026rdquo;\n}\n# Get the Module versions Install-Module Configuration -Scope CurrentUser -Force $Modules = Get-ManifestValue -Path .\\dbachecks.psd1 -PropertyName RequiredModules\n$PesterVersion = $Modules.Where{$.Get_Item(\u0026lsquo;ModuleName\u0026rsquo;) -eq \u0026lsquo;Pester\u0026rsquo;}[0].Get_Item(\u0026lsquo;ModuleVersion\u0026rsquo;) $PSFrameworkVersion = $Modules.Where{$.Get_Item(\u0026lsquo;ModuleName\u0026rsquo;) -eq \u0026lsquo;PSFramework\u0026rsquo;}[0].Get_Item(\u0026lsquo;ModuleVersion\u0026rsquo;) $dbatoolsVersion = $Modules.Where{$_.Get_Item(\u0026lsquo;ModuleName\u0026rsquo;) -eq \u0026lsquo;dbatools\u0026rsquo;}[0].Get_Item(\u0026lsquo;ModuleVersion\u0026rsquo;)\n# Install Pester try { Write-Output \u0026ldquo;Installing Pester\u0026rdquo; Install-Module Pester -RequiredVersion $PesterVersion -Scope CurrentUser -Force -SkipPublisherCheck Write-Output \u0026ldquo;Installed Pester\u0026rdquo;\n} catch { Write-Error \u0026ldquo;Failed to Install Pester $($_)\u0026rdquo; } # Install dbatools try { Write-Output \u0026ldquo;Installing PSFramework\u0026rdquo; Install-Module PSFramework -RequiredVersion $PsFrameworkVersion -Scope CurrentUser -Force Write-Output \u0026ldquo;Installed PSFramework\u0026rdquo;\n} catch { Write-Error \u0026ldquo;Failed to Install PSFramework $($_)\u0026rdquo; } # Install dbachecks try { Write-Output \u0026ldquo;Installing dbatools\u0026rdquo; Install-Module dbatools -RequiredVersion $dbatoolsVersion -Scope CurrentUser -Force Write-Output \u0026ldquo;Installed dbatools\u0026rdquo;\n} catch { Write-Error \u0026ldquo;Failed to Install dbatools $($_)\u0026rdquo; }\n# Add current folder to PSModulePath try { Write-Output \u0026ldquo;Adding local folder to PSModulePath\u0026rdquo; $ENV:PSModulePath = $ENV:PSModulePath + \u0026ldquo;;$pwd\u0026rdquo; Write-Output \u0026ldquo;Added local folder to PSModulePath\u0026rdquo; $ENV:PSModulePath.Split(\u0026rsquo;;\u0026rsquo;) } catch { Write-Error \u0026ldquo;Failed to add $pwd to PSModulePAth - $_\u0026rdquo; }\nI use the Configuration module from Joel Bennett to get the required module versions for the required modules and then add the path to $ENV:PSModulePath so that the modules will be imported. I think this is because the modules did not import correctly without it.\nOnce I have the modules I can then run Pester as follows\ntry { Write-Output \u0026ldquo;Installing dbachecks\u0026rdquo; Import-Module .\\dbachecks.psd1 Write-Output \u0026ldquo;Installed dbachecks\u0026rdquo;\n} catch { Write-Error \u0026ldquo;Failed to Install dbachecks $($_)\u0026rdquo; } $TestResults = Invoke-Pester .\\tests -ExcludeTag Integration,IntegrationTests -Show None -OutputFile $(Build.SourcesDirectory)\\Test-Pester.XML -OutputFormat NUnitXml -PassThru\nif ($TestResults.failedCount -ne 0) { Write-Error \u0026ldquo;Pester returned errors\u0026rdquo; }\nAs you can see I import the dbachecks module from the local folder, run Invoke-Pester and output the results to an XML file and check that there are no failing tests.\nWhether you use the task or PowerShell the next step is to Publish the test results so that they are displayed in the build results in VSTS.\nClick on the + sign next to Phase 1 and search for Publish\nChoose the Publish Test Results task and leave everything as default unless you have renamed the xml file. This means that on the summary page you will see some test results\nand on the tests tab you can see more detailed information and drill down into the tests\nTrigger The next step is to trigger a build when a commit is pushed to the development branch. Click on Triggers and tick enable continuous integration\nSaving the Build Definition I would normally save the build definition regularly and ensure that there is a good message in the comment. I always tell clients that this is like a commit message for your build process so that you can see the history of the changes for the build definition.\nYou can see the history on the edit tab of the build definition\nIf you want to compare or revert the build definition this can be done using the hamburger menu as shown below.\nUpdate the Module Version Now we need to create a build definition for the master branch to update the module version and sign the code ready for publishing to the PowerShell Gallery when we commit or merge to master\nCreate a new build definition as above but this time choose the master branch\nAgain choose an empty process and name it sensibly, click the + sign next to Phase 1 and search for PowerShell\nI change the version to 2 and use this code. Note that the commit message has ***NO_CI*** in it. Putting this in a commit message tells VSTS not to trigger a build for this commit.\n$manifest = Import-PowerShellDataFile .\\dbachecks.psd1 [version]$version = $Manifest.ModuleVersion Write-Output \u0026ldquo;Old Version - $Version\u0026rdquo; # Add one to the build of the version number [version]$NewVersion = \u0026ldquo;{0}.{1}.{2}\u0026rdquo; -f $Version.Major, $Version.Minor, ($Version.Build + 1) Write-Output \u0026ldquo;New Version - $NewVersion\u0026rdquo; # Update the manifest file try { Write-Output \u0026ldquo;Updating the Module Version to $NewVersion\u0026rdquo; $path = \u0026ldquo;$pwd\\dbachecks.psd1\u0026rdquo; (Get-Content .\\dbachecks.psd1) -replace $version, $NewVersion | Set-Content .\\dbachecks.psd1 -Encoding string Write-Output \u0026ldquo;Updated the Module Version to $NewVersion\u0026rdquo; } catch { Write-Error \u0026ldquo;Failed to update the Module Version - $_\u0026rdquo; }\ntry { Write-Output \u0026ldquo;Updating GitHub\u0026rdquo; git config user.email \u0026ldquo;mrrobsewell@outlook.com\u0026rdquo; git config user.name \u0026ldquo;SQLDBAWithABeard\u0026rdquo; git add .\\dbachecks.psd1 git commit -m \u0026ldquo;Updated Version Number to $NewVersion ***NO_CI***\u0026rdquo;\ngit push https://$(RobsGitHubPAT)@github.com/dataplat/dbachecks.git HEAD:master Write-Output \u0026ldquo;Updated GitHub \u0026quot;\n} catch { $_ | Fl -Force Write-Output \u0026ldquo;Failed to update GitHub\u0026rdquo; }\nI use Get-Content Set-Content as I had errors with the Update-ModuleManifest but Adam Murray g | t uses this code to update the version using the BuildID from VSTS\n$newVersion = New-Object version -ArgumentList 1, 0, 0, $env:BUILD_BUILDID $Public = @(Get-ChildItem -Path $ModulePath\\Public\\*.ps1) $Functions = $public.basename Update-ModuleManifest -Path $ModulePath\\$ModuleName.psd1 -ModuleVersion $newVersion -FunctionsToExport $Functions\nYou can commit your change by adding your PAT token as a variable under the variables tab. Don’t forget to tick the padlock to make it a secret so it is not displayed in the logs\nSign the code with a certificate The SQL Collaborative uses a code signing certificate from DigiCert who allow MVPs to use one for free to sign their code for open source projects, Thank You. We had to upload the certificate to the secure files store in the VSTS library. Click on library, secure files and the blue +Secure File button\nYou also need to add the password as a variable under the variables tab as above. Again don’t forget to tick the padlock to make it a secret so it is not displayed in the logs\nThen you need to add a task to download the secure file. Click on the + sign next to Phase 1 and search for secure\nchoose the file from the drop down\nNext we need to import the certificate and sign the code. I use a PowerShell task for this with the following code\n$ErrorActionPreference = \u0026lsquo;Stop\u0026rsquo; # read in the certificate from a pre-existing PFX file # I have checked this with @IISResetMe and this does not go in the store only memory $cert = [System.Security.Cryptography.X509Certificates.X509Certificate2]::new(\u0026quot;$(Agent.WorkFolder)\\_temp\\dbatools-code-signing-cert.pfx\u0026rdquo;,\u0026quot;$(CertPassword)\u0026quot;)\ntry { Write-Output \u0026ldquo;Signing Files\u0026rdquo; # find all scripts in your module\u0026hellip; Get-ChildItem -Filter *.ps1 -Include *.ps1 -Recurse -ErrorAction SilentlyContinue | # \u0026hellip;that do not have a signature yet\u0026hellip; Where-Object { ($_ | Get-AuthenticodeSignature).Status -eq \u0026lsquo;NotSigned\u0026rsquo; } | # and apply one # (note that we added -WhatIf so no signing occurs. Remove this only if you # really want to add digital signatures!) Set-AuthenticodeSignature -Certificate $cert Write-Output \u0026ldquo;Signed Files\u0026rdquo; } catch { $_ | Format-List -Force Write-Error \u0026ldquo;Failed to sign scripts\u0026rdquo; }\nwhich will import the certificate into memory and sign all of the scripts in the module folder.\nPublish your artifact The last step of the master branch build publishes the artifact (your signed module) to VSTS ready for the release task. Again, click the + sign next to Phase one and choose the Publish Artifact task not the deprecated copy and publish artifact task and give the artifact a useful name\nDon’t forget to set the trigger for the master build as well following the same steps as the development build above\nPublish to the PowerShell Gallery Next we create a release to trigger when there is an artifact ready and publish to the PowerShell Gallery.\nClick the Releases tab and New Definition\nChoose an empty process and name the release definition appropriately\nNow click on the artifact and choose the master build definition. If you have not run a build you will get an error like below but dont worry click add.\nClick on the lightning bolt next to the artifact to open the continuous deployment trigger\nand turn on Continuous Deployment so that when an artifact has been created with an updated module version and signed code it is published to the gallery\nNext, click on the environment and name it appropriately and then click on the + sign next to Agent Phase and choose a PowerShell step\nYou may wonder why I dont choose the PowerShell Gallery Packager task. There are two reasons. First I need to install the required modules for dbachecks (dbatools, PSFramework, Pester) prior to publishing and second it appears that the API Key is stored in plain text\nI save my API key for the PowerShell Gallery as a variable again making sure to tick the padlock to make it a secret\nand then use the following code to install the required modules and publish the module to the gallery\nInstall-Module dbatools -Scope CurrentUser -Force Install-Module Pester -Scope CurrentUser -SkipPublisherCheck -Force Install-Module PSFramework -Scope CurrentUser -Force\nPublish-Module -Path \u0026ldquo;$(System.DefaultWorkingDirectory)/Master - Version Update, Signing and Publish Artifact/dbachecks\u0026rdquo; -NuGetApiKey \u0026ldquo;$(GalleryApiKey)\u0026rdquo;\nThats it 🙂\nNow we have a process that will automatically run our Pester tests when we commit or merge to the development branch and then update our module version number and sign our code and publish to the PowerShell Gallery when we commit or merge to the master branch\nAdded Extra – Dashboard I like to create dashboards in VSTS to show the progress of the various definitions. You can do this under the dashboard tab. Click edit and choose or search for widgets and add them to the dashboard\nAdded Extra – Badges You can also enable badges for displaying on your readme in GitHub (or VSTS). For the build defintions this is under the options tab.\nfor the release definitions, click the environment and then options and integrations\nYou can then copy the URL and use it in your readme like this on dbachecks\nThe SQL Collaborative has joined the preview of enabling public access to VSTS projects as detailed in this blog post So you can see the dbachecks build and release without the need to log in and soon the dbatools process as well\nI hope you found this useful and if you have any questions or comments please feel free to contact me\nHappy Automating!\n","date":"2018-05-01T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/05/32-Dashboard.png","permalink":"https://dbachecks.io/blog/version-update-code-signing-and-publishing-to-the-powershell-gallery-with-vsts/","title":"Version Update, Code Signing and publishing to the PowerShell Gallery with VSTS"},{"content":"It’s been 45 days since we released dbachecks\nSince then there have been 25 releases to the PowerShell Gallery!! Today release 1.1.119 was released 🙂 There have been over 2000 downloads of the module already.\nIn the beginning we had 80 checks and 108 configuration items, today we have 84 checks and 125 configuration items!\nIf you have already installed dbachecks it is important to make sure that you update regularly. You can do this by running\nUpdate-Module dbachecks\nIf you want to try dbachecks, you can install it from the PowerShell Gallery by running\nInstall-Module dbachecks # -Scope CurrentUser # if not running as admin\nYou can read more about installation and read a number of blog posts about using different parts of dbachecks at this link https://dbatools.io/installing-dbachecks/\nHADR Tests Today we updated the HADR tests to add the capability to test multiple availability groups and fix a couple of bugs\nOnce you have installed dbachecks you will need to set some configuration so that you can perform the tests. You can see all of the configuration items and their values using\nGet-DbcConfig | Out-GridView\nYou can set the values with the Set-DbcConfig command. It has intellisense to make things easier 🙂 To set the values for the HADR tests\nSet-DbcConfig -Name app.cluster -Value sql1 Set-DbcConfig -Name app.computername -Value sql0,sql1 Set-DbcConfig -Name app.sqlinstance -Value sql0,sql1 Set-DbcConfig -Name domain.name -Value TheBeard.Local Set-DbcConfig -Name skip.hadr.listener.pingcheck -Value $true\napp.cluster requires one of the nodes of the cluster. app.computername requires the windows computer names of the machines to run operating system checks against app.sqlinstance requires the instance names of the SQL instances that you want to run SQL checks against (These are default instances but it will accept SERVER\\INSTANCE) domain.name requires the domain name the machines are part of skip.hadr.listener.pingcheck is a boolean value which defines whether to skip the listener ping check or not. As this is in Azure I am skipping the check by setting the value to $true policy.hadr.tcpport is set to default to 1433 but you can also set this configuration if your SQL is using a different port NOTE – You can find all the configuration items that can skip tests by running\nGet-DbcConfig -Name skip*\nNow we have set the configuration (For the HADR checks – There are many more configurations for other checks that you can set) you can run the checks with\nInvoke-DbcCheck -Check HADR\nThis runs the following checks\nEach node on the cluster should be up Each resource on the cluster should be online Each SQL instance should be enabled for Always On Connection check for the listener and each node Should be pingable (unless skip.hadr.listener.pingcheck is set to true) Should be able to run SQL commands Should be the correct domain name Should be using the correct tcpport Each replica should not be in unknown state Each synchronous replica should be synchronised Each asynchronous replica should be synchonising Each database should be synchronised (or synchronising) on each replica Each database should be failover ready on each replica Each database should be joined to the availability group on each replica Each database should not be suspended on each replica Each node should have the AlwaysOn_Health extended event Each node should have the AlwaysOn_Health extended event running Each node should have the AlwaysOn_Health extended event set to auto start (Apologies folk over the pond, I use the Queens English 😉 )\nThis is good for us to be able to run this check at the command line but we can do more.\nWe can export the results and display them with PowerBi. Note we need to add -PassThru so that the results go through the pipeline and that I used -Show Fails so that only the titles of the Describe and Context blocks and any failing tests are displayed to the screen\nInvoke-DbcCheck -Check HADR -Show Fails -PassThru | Update-DbcPowerBiDataSource -Environment HADR-Test Start-DbcPowerBi\nThis will create a file at C:\\Windows\\Temp\\dbachecks and open the PowerBi report. You will need to refresh the data in the report and then you will see\nExcellent, everything passed 🙂\nSaving Configuration for reuse We can save our configuration using Export-DbcConfig which will export the configuration to a json file\nExport-DbcConfig -Path Git:\\PesterTests\\MyHADRTestsForProd.json\nso that we can run this particular set of tests with this comfiguration by importing the configuration using Import-DbcConfig\nImport-DbcConfig -Path -Path Git:\\PesterTests\\MyHADRTestsForProd.json Invoke-DbcCheck -Check HADR\nIn this way you can set up different check configurations for different use cases. This also enables you to make use of the checks in your CI/CD process. For example, I have a GitHub repository for creating a domain, a cluster and a SQL 2017 availability group using VSTS. I have saved a dbachecks configuration to my repository and as part of my build I can import that configuration, run the checks and output them to XML for consumption by the publish test results task of VSTS\nAfter copying the configuration to the machine, I run\nImport-Dbcconfig -Path C:\\Windows\\Temp\\FirstBuild.json Invoke-DbcCheck-AllChecks -OutputFile PesterTestResultsdbachecks.xml -OutputFormat NUnitXml\nin my build step and then use the publish test results task and VSTS does the rest 🙂\n","date":"2018-04-08T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/04/VSTS-results.png","permalink":"https://dbachecks.io/blog/checking-availability-groups-with-dbachecks/","title":"Checking Availability Groups with dbachecks"},{"content":"For the last couple of months members of the dbatools team have been working on a new PowerShell module called dbachecks. This open source PowerShell module will enable you to validate your SQL Instances. Today it is released for you all to start to use 🙂\nValidate Your SQL Instances? What do I mean by validate your SQL Instances? You want to know if your SQL Instances are (still) set up in the way that you want them to be or that you have not missed any configurations when setting them up. With dbachecks you can use any or all of the 80 checks to ensure one or many SQL Instances are as you want them to be. Using Pester, dbachecks will validate your SQL Instance(s) against default settings or ones that you configure yourself.\nInstallation Installation is via the PowerShell Gallery. You will need to open PowerShell on a machine connected to the internet and run\nInstall-Module dbachecks\nIf you are not running your process as admin or you only want (or are able) to install for your own user account you will need to\nInstall-Module -Scope CurrentUser\nThis will also install the PSFramework module used for configuration (and other things beneath the hood) and the latest version (4.2.0 – released on Sunday!) of Pester\nOnce you have installed the module you can see the commands available by running\nGet-Command -Module dbachecks\nTo be able to use these (and any PowerShell) commands, your first step should always be Get-Help\nGet-Help Send-DbcMailMessage\n80 Checks At the time of release, dbachecks has 80 checks. You can see all of the checks by running\nGet-DbcCheck\n(Note this has nothing to do with DBCC CheckDb!) Here is the output of\nGet-DbcCheck | Select Group, UniqueTag\nso you can see the current checks\nGroup UniqueTag Agent AgentServiceAccount Agent DbaOperator Agent FailsafeOperator Agent DatabaseMailProfile Agent FailedJob Database DatabaseCollation Database SuspectPage Database TestLastBackup Database TestLastBackupVerifyOnly Database ValidDatabaseOwner Database InvalidDatabaseOwner Database LastGoodCheckDb Database IdentityUsage Database RecoveryModel Database DuplicateIndex Database UnusedIndex Database DisabledIndex Database DatabaseGrowthEvent Database PageVerify Database AutoClose Database AutoShrink Database LastFullBackup Database LastDiffBackup Database LastLogBackup Database VirtualLogFile Database LogfileCount Database LogfileSize Database FileGroupBalanced Database AutoCreateStatistics Database AutoUpdateStatistics Database AutoUpdateStatisticsAsynchronously Database DatafileAutoGrowthType Database Trustworthy Database OrphanedUser Database PseudoSimple Database AdHocWorkloads Domain DomainName Domain OrganizationalUnit HADR ClusterHealth HADR ClusterServerHealth HADR HADR System.Object[] Instance SqlEngineServiceAccount Instance SqlBrowserServiceAccount Instance TempDbConfiguration Instance AdHocWorkload Instance BackupPathAccess Instance DAC Instance NetworkLatency Instance LinkedServerConnection Instance MaxMemory Instance OrphanedFile Instance ServerNameMatch Instance MemoryDump Instance SupportedBuild Instance SaRenamed Instance DefaultBackupCompression Instance XESessionStopped Instance XESessionRunning Instance XESessionRunningAllowed Instance OLEAutomation Instance WhoIsActiveInstalled LogShipping LogShippingPrimary LogShipping LogShippingSecondary Server PowerPlan Server InstanceConnection Server SPN Server DiskCapacity Server PingComputer MaintenancePlan SystemFull MaintenancePlan UserFull MaintenancePlan UserDiff MaintenancePlan UserLog MaintenancePlan CommandLog MaintenancePlan SystemIntegrityCheck MaintenancePlan UserIntegrityCheck MaintenancePlan UserIndexOptimize MaintenancePlan OutputFileCleanup MaintenancePlan DeleteBackupHistory MaintenancePlan PurgeJobHistory 108 Configurations One of the things I have been talking about in my presentation “Green is Good Red is Bad” is configuring Pester checks so that you do not have to keep writing new tests for the same thing but with different values.\nFor example, a different user for a database owner. The code to write the test for the database owner is the same but the value might be different for different applications, environments, clients, teams, domains etc. I gave a couple of different methods for achieving this.\nWith dbachecks we have made this much simpler enabling you to set configuration items at run-time or for your session and enabling you to export and import them so you can create different configs for different use cases\nThere are 108 configuration items at present. You can see the current configuration by running\nGet-DbcConfig\nwhich will show you the name of the config, the value it is currently set and the description\nYou can see all of the configs and their descriptions here\nName Description agent.databasemailprofile Name of the Database Mail Profile in SQL Agent agent.dbaoperatoremail Email address of the DBA Operator in SQL Agent agent.dbaoperatorname Name of the DBA Operator in SQL Agent agent.failsafeoperator Email address of the DBA Operator in SQL Agent app.checkrepos Where Pester tests/checks are stored app.computername List of Windows Servers that Windows-based tests will run against app.localapp Persisted files live here app.maildirectory Files for mail are stored here app.sqlcredential The universal SQL credential if Trusted/Windows Authentication is not used app.sqlinstance List of SQL Server instances that SQL-based tests will run against app.wincredential The universal Windows if default Windows Authentication is not used command.invokedbccheck.excludecheck Invoke-DbcCheck: The checks that should be skipped by default. domain.domaincontroller The domain controller to process your requests domain.name The Active Directory domain that your server is a part of domain.organizationalunit The OU that your server should be a part of mail.failurethreshhold Number of errors that must be present to generate an email report mail.from Email address the email reports should come from mail.smtpserver Store the name of the smtp server to send email reports mail.subject Subject line of the email report mail.to Email address to send the report to policy.backup.datadir Destination server data directory policy.backup.defaultbackupcompreesion Default Backup Compression check should be enabled $true or disabled $false policy.backup.diffmaxhours Maxmimum number of hours before Diff Backups are considered outdated policy.backup.fullmaxdays Maxmimum number of days before Full Backups are considered outdated policy.backup.logdir Destination server log directory policy.backup.logmaxminutes Maxmimum number of minutes before Log Backups are considered outdated policy.backup.newdbgraceperiod The number of hours a newly created database is allowed to not have backups policy.backup.testserver Destination server for backuptests policy.build.warningwindow The number of months prior to a build being unsupported that you want warning about policy.connection.authscheme Auth requirement (Kerberos, NTLM, etc) policy.connection.pingcount Number of times to ping a server to establish average response time policy.connection.pingmaxms Maximum response time in ms policy.dacallowed DAC should be allowed $true or disallowed $false policy.database.autoclose Auto Close should be allowed $true or dissalowed $false policy.database.autocreatestatistics Auto Create Statistics should be enabled $true or disabled $false policy.database.autoshrink Auto Shrink should be allowed $true or dissalowed $false policy.database.autoupdatestatistics Auto Update Statistics should be enabled $true or disabled $false policy.database.autoupdatestatisticsasynchronously Auto Update Statistics Asynchronously should be enabled $true or disabled $false policy.database.filebalancetolerance Percentage for Tolerance for checking for balanced files in a filegroups policy.database.filegrowthexcludedb Databases to exclude from the file growth check policy.database.filegrowthtype Growth Type should be \u0026lsquo;kb\u0026rsquo; or \u0026lsquo;percent\u0026rsquo; policy.database.filegrowthvalue The auto growth value (in kb) should be equal or higher than this value. Example: A value of 65535 means at least 64MB. policy.database.logfilecount The number of Log files expected on a database policy.database.logfilesizecomparison How to compare data and log file size, options are maximum or average policy.database.logfilesizepercentage Maximum percentage of Data file Size that logfile is allowed to be. policy.database.maxvlf Max virtual log files policy.dbcc.maxdays Maxmimum number of days before DBCC CHECKDB is considered outdated policy.diskspace.percentfree Percent disk free policy.dump.maxcount Maximum number of expected dumps policy.hadr.tcpport The TCPPort for the HADR check policy.identity.usagepercent Maxmimum percentage of max of identity column policy.invaliddbowner.excludedb Databases to exclude from invalid dbowner checks policy.invaliddbowner.name The database owner account should not be this user policy.network.latencymaxms Max network latency average policy.ola.commandlogenabled Ola\u0026rsquo;s CommandLog Cleanup should be enabled $true or disabled $false policy.ola.commandlogscheduled Ola\u0026rsquo;s CommandLog Cleanup should be scheduled $true or disabled $false policy.ola.database The database where Ola\u0026rsquo;s maintenance solution is installed policy.ola.deletebackuphistoryenabled Ola\u0026rsquo;s Delete Backup History should be enabled $true or disabled $false policy.ola.deletebackuphistoryscheduled Ola\u0026rsquo;s Delete Backup History should be scheduled $true or disabled $false policy.ola.installed Checks to see if Ola Hallengren solution is installed policy.ola.outputfilecleanupenabled Ola\u0026rsquo;s Output File Cleanup should be enabled $true or disabled $false policy.ola.outputfilecleanupscheduled Ola\u0026rsquo;s Output File Cleanup should be scheduled $true or disabled $false policy.ola.purgejobhistoryenabled Ola\u0026rsquo;s Purge Job History should be enabled $true or disabled $false policy.ola.purgejobhistoryscheduled Ola\u0026rsquo;s Purge Job History should be scheduled $true or disabled $false policy.ola.systemfullenabled Ola\u0026rsquo;s Full System Database Backup should be enabled $true or disabled $false policy.ola.systemfullretention Ola\u0026rsquo;s Full System Database Backup retention number of hours policy.ola.systemfullscheduled Ola\u0026rsquo;s Full System Database Backup should be scheduled $true or disabled $false policy.ola.systemintegritycheckenabled Ola\u0026rsquo;s System Database Integrity should be enabled $true or disabled $false policy.ola.systemintegritycheckscheduled Ola\u0026rsquo;s System Database Integrity should be scheduled $true or disabled $false policy.ola.userdiffenabled Ola\u0026rsquo;s Diff User Database Backup should be enabled $true or disabled $false policy.ola.userdiffretention Ola\u0026rsquo;s Diff User Database Backup retention number of hours policy.ola.userdiffscheduled Ola\u0026rsquo;s Diff User Database Backup should be scheduled $true or disabled $false policy.ola.userfullenabled Ola\u0026rsquo;s Full User Database Backup should be enabled $true or disabled $false policy.ola.userfullretention Ola\u0026rsquo;s Full User Database Backup retention number of hours policy.ola.userfullscheduled Ola\u0026rsquo;s Full User Database Backup should be scheduled $true or disabled $false policy.ola.userindexoptimizeenabled Ola\u0026rsquo;s User Index Optimization should be enabled $true or disabled $false policy.ola.userindexoptimizescheduled Ola\u0026rsquo;s User Index Optimization should be scheduled $true or disabled $false policy.ola.userintegritycheckenabled Ola\u0026rsquo;s User Database Integrity should be enabled $true or disabled $false policy.ola.userintegritycheckscheduled Ola\u0026rsquo;s User Database Integrity should be scheduled $true or disabled $false policy.ola.userlogenabled Ola\u0026rsquo;s Log User Database Backup should be enabled $true or disabled $false policy.ola.userlogretention Ola\u0026rsquo;s Log User Database Backup retention number of hours policy.ola.userlogscheduled Ola\u0026rsquo;s Log User Database Backup should be scheduled $true or disabled $false policy.oleautomation OLE Automation should be enabled $true or disabled $false policy.pageverify Page verify option should be set to this value policy.recoverymodel.excludedb Databases to exclude from standard recovery model check policy.recoverymodel.type Standard recovery model policy.storage.backuppath Enables tests to check if servers have access to centralized backup location policy.validdbowner.excludedb Databases to exclude from valid dbowner checks policy.validdbowner.name The database owner account should be this user policy.whoisactive.database Which database should contain the sp_WhoIsActive stored procedure policy.xevent.requiredrunningsession List of XE Sessions that should be running. policy.xevent.requiredstoppedsession List of XE Sessions that should not be running. policy.xevent.validrunningsession List of XE Sessions that can be be running. skip.backup.testing Don\u0026rsquo;t run Test-DbaLastBackup by default (it\u0026rsquo;s not read-only) skip.connection.ping Skip the ping check for connectivity skip.connection.remoting Skip PowerShell remoting check for connectivity skip.database.filegrowthdisabled Skip validation of datafiles which have growth value equal to zero. skip.database.logfilecounttest Skip the logfilecount test skip.datafilegrowthdisabled Skip validation of datafiles which have growth value equal to zero. skip.dbcc.datapuritycheck Skip data purity check in last good dbcc command skip.diffbackuptest Skip the Differential backup test skip.logfilecounttest Skip the logfilecount test skip.logshiptesting Skip the logshipping test skip.tempdb1118 Don\u0026rsquo;t run test for Trace Flag 1118 skip.tempdbfilecount Don\u0026rsquo;t run test for Temp Database File Count skip.tempdbfilegrowthpercent Don\u0026rsquo;t run test for Temp Database File Growth in Percent skip.tempdbfilesizemax Don\u0026rsquo;t run test for Temp Database Files Max Size skip.tempdbfilesonc Don\u0026rsquo;t run test for Temp Database Files on C Running A Check You can quickly run a single check by calling Invoke-DbcCheck.\nInvoke-DbcCheck -SqlInstance localhost -Check FailedJob\nExcellent, my agent jobs have not failed 🙂\nInvoke-DbcCheck -SqlInstance localhost -Check LastGoodCheckDb\nThats good, all of my databases have had a successful DBCC CHECKDB within the last 7 days.\nSetting a Configuration To save me from having to specify the instance I want to run my tests against I can set the app.sqlinstance config to the instances I want to check.\nSet-DbcConfig -Name app.sqlinstance -Value localhost, \u0026rsquo;localhost\\PROD1\u0026rsquo;\nThen whenever I call Invoke-DbcCheck it will run against those instances for the SQL checks\nSo now if I run\nInvoke-DbcCheck -Check LastDiffBackup\nI can see that I dont have a diff backup for the databases on both instances. Better stop writing this and deal with that !!\nThe configurations are stored in the registry but you can export them and then import them for re-use easily. I have written another blog post about that.\nThe Show Parameter Getting the results of the tests on the screen is cool but if you are running a lot of tests against a lot of instances then you might find that you have 3 failed tests out of 15000! This will mean a lot of scrolling through green text looking for the red text and you may find that your PowerShell buffer doesnt hold all of your test results leaving you very frustrated.\ndbachecks supports the Pester Show parameter enabling you to filter the output of the results to the screen. The available values are Summary, None, Fails, Inconclusive, Passed, Pending and Skipped\nin my opinion by far the most useful one is Fails as this will show you only the failed tests with the context to enable you to see which tests have failed\nInvoke-DbcCheck -Check Agent -Show Fails\nIf we check all of the checks tagged as Agent we can easily see that most passed but The Job That Fails (surprisingly) failed. All of the other tests that were run for the agent service, operators, failsafe operator, database mail and all other agent jobs all passed in the example below\nTest Results are for other People as well It is all very well and good being able to run tests and get the results on our screen. It will be very useful for people to be able to validate a new SQL instance for example or run a morning check or the first step of an incident response. But test results are also useful for other people so we need to be able to share them\nWe have created a Power Bi Dashboard that comes with the dbachecks module to enable easy sharing of the test results. You can also send the results via email using Send-DbcMailMessage. we have an open issue for putting them into a database that we would love you to help resolve.\nTo get the results into PowerBi you can run\nInvoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment Production\nThis will run all of the dbachecks using your configuration for your Production environment, output only the failed tests to the screen and save the results in your windows\\temp\\dbachecks folder with a suffix of Production\nIf you then used a different configuration for your development environment and ran\nInvoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment Development\nit will run all of the dbachecks using your configuration for your Development environment, output only the failed tests to the screen and save the results in your windows\\temp\\dbachecks folder with a suffix of Development and you would end up with two files in the folder\nYou can then simply run\nStart-DbcPowerBi\nand as long as you have the (free) Powerbi Desktop then you will see this. You will need to refresh the data to get your test results\nOf course it is Powerbi so you can publish this report. Here it is so that you can click around and see what it looks like\nIt’s Open Source – We Want Your Ideas, Issues, New Code dbachecks is open-source available on GitHub for anyone to contribute\nWe would love you to contribute. Please open issues for new tests, enhancements, bugs. Please fork the repository and add code to improve the module. please give feedback to make this module even more useful\nYou can also come in the SQL Server Community Slack and join the dbachecks channel and get advice, make comments or just join in the conversation\nFurther Reading There are many more introduction blog posts covering different areas at\ndbachecks.io/install Thank You I want to say thank you to all of the people who have enabled dbachecks to get this far. These wonderful people have used their own time to ensure that you have a useful tool available to you for free\nChrissy Lemaire @cl\nFred Weinmann @FredWeinmann\nCláudio Silva @ClaudioESSilva\nStuart Moore @napalmgram\nShawn Melton @wsmelton\nGarry Bargsley @gbargsley\nStephen Bennett @staggerlee011\nSander Stad @SQLStad\nJess Pomfret @jpomfret\nJason Squires @js0505\nShane O’Neill @SOZDBA\nTony Wilhelm @TonyWSQL\nand all of the other people who have contributed in the dbachecks Slack channel\n","date":"2018-02-22T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/02/09-PowerBi.png","permalink":"https://dbachecks.io/blog/announcing-dbachecks-configurable-powershell-validation-for-your-sql-instances/","title":"Announcing dbachecks – Configurable PowerShell Validation For Your SQL Instances"},{"content":"Today is the day that we have announced dbachecks a PowerShell module enabling you to validate your SQL Instances. You can read more about it here where you can learn how to install it and see some simple use cases\n108 Configurations One of the things I have been talking about in my presentation “Green is Good Red is Bad” is configuring Pester checks so that you do not have to keep writing new tests for the same thing but with different values.\nFor example, a different user for a database owner. The code to write the test for the database owner is the same but the value might be different for different applications, environments, clients, teams, domains etc. I gave a couple of different methods for achieving this.\nWith dbachecks we have made this much simpler enabling you to set configuration items at run-time or for your session and enabling you to export and import them so you can create different configs for different use cases\nThere are 108 configuration items at present. You can see the current configuration by running\nGet-DbcConfig\nwhich will show you the name of the config, the value it is currently set and the description You can see all of the configs and their descriptions here\nName Description agent.databasemailprofile Name of the Database Mail Profile in SQL Agent agent.dbaoperatoremail Email address of the DBA Operator in SQL Agent agent.dbaoperatorname Name of the DBA Operator in SQL Agent agent.failsafeoperator Email address of the DBA Operator in SQL Agent app.checkrepos Where Pester tests/checks are stored app.computername List of Windows Servers that Windows-based tests will run against app.localapp Persisted files live here app.maildirectory Files for mail are stored here app.sqlcredential The universal SQL credential if Trusted/Windows Authentication is not used app.sqlinstance List of SQL Server instances that SQL-based tests will run against app.wincredential The universal Windows if default Windows Authentication is not used command.invokedbccheck.excludecheck Invoke-DbcCheck: The checks that should be skipped by default. domain.domaincontroller The domain controller to process your requests domain.name The Active Directory domain that your server is a part of domain.organizationalunit The OU that your server should be a part of mail.failurethreshhold Number of errors that must be present to generate an email report mail.from Email address the email reports should come from mail.smtpserver Store the name of the smtp server to send email reports mail.subject Subject line of the email report mail.to Email address to send the report to policy.backup.datadir Destination server data directory policy.backup.defaultbackupcompreesion Default Backup Compression check should be enabled $true or disabled $false policy.backup.diffmaxhours Maxmimum number of hours before Diff Backups are considered outdated policy.backup.fullmaxdays Maxmimum number of days before Full Backups are considered outdated policy.backup.logdir Destination server log directory policy.backup.logmaxminutes Maxmimum number of minutes before Log Backups are considered outdated policy.backup.newdbgraceperiod The number of hours a newly created database is allowed to not have backups policy.backup.testserver Destination server for backuptests policy.build.warningwindow The number of months prior to a build being unsupported that you want warning about policy.connection.authscheme Auth requirement (Kerberos, NTLM, etc) policy.connection.pingcount Number of times to ping a server to establish average response time policy.connection.pingmaxms Maximum response time in ms policy.dacallowed DAC should be allowed $true or disallowed $false policy.database.autoclose Auto Close should be allowed $true or dissalowed $false policy.database.autocreatestatistics Auto Create Statistics should be enabled $true or disabled $false policy.database.autoshrink Auto Shrink should be allowed $true or dissalowed $false policy.database.autoupdatestatistics Auto Update Statistics should be enabled $true or disabled $false policy.database.autoupdatestatisticsasynchronously Auto Update Statistics Asynchronously should be enabled $true or disabled $false policy.database.filebalancetolerance Percentage for Tolerance for checking for balanced files in a filegroups policy.database.filegrowthexcludedb Databases to exclude from the file growth check policy.database.filegrowthtype Growth Type should be \u0026lsquo;kb\u0026rsquo; or \u0026lsquo;percent\u0026rsquo; policy.database.filegrowthvalue The auto growth value (in kb) should be equal or higher than this value. Example: A value of 65535 means at least 64MB. policy.database.logfilecount The number of Log files expected on a database policy.database.logfilesizecomparison How to compare data and log file size, options are maximum or average policy.database.logfilesizepercentage Maximum percentage of Data file Size that logfile is allowed to be. policy.database.maxvlf Max virtual log files policy.dbcc.maxdays Maxmimum number of days before DBCC CHECKDB is considered outdated policy.diskspace.percentfree Percent disk free policy.dump.maxcount Maximum number of expected dumps policy.hadr.tcpport The TCPPort for the HADR check policy.identity.usagepercent Maxmimum percentage of max of identity column policy.invaliddbowner.excludedb Databases to exclude from invalid dbowner checks policy.invaliddbowner.name The database owner account should not be this user policy.network.latencymaxms Max network latency average policy.ola.commandlogenabled Ola\u0026rsquo;s CommandLog Cleanup should be enabled $true or disabled $false policy.ola.commandlogscheduled Ola\u0026rsquo;s CommandLog Cleanup should be scheduled $true or disabled $false policy.ola.database The database where Ola\u0026rsquo;s maintenance solution is installed policy.ola.deletebackuphistoryenabled Ola\u0026rsquo;s Delete Backup History should be enabled $true or disabled $false policy.ola.deletebackuphistoryscheduled Ola\u0026rsquo;s Delete Backup History should be scheduled $true or disabled $false policy.ola.installed Checks to see if Ola Hallengren solution is installed policy.ola.outputfilecleanupenabled Ola\u0026rsquo;s Output File Cleanup should be enabled $true or disabled $false policy.ola.outputfilecleanupscheduled Ola\u0026rsquo;s Output File Cleanup should be scheduled $true or disabled $false policy.ola.purgejobhistoryenabled Ola\u0026rsquo;s Purge Job History should be enabled $true or disabled $false policy.ola.purgejobhistoryscheduled Ola\u0026rsquo;s Purge Job History should be scheduled $true or disabled $false policy.ola.systemfullenabled Ola\u0026rsquo;s Full System Database Backup should be enabled $true or disabled $false policy.ola.systemfullretention Ola\u0026rsquo;s Full System Database Backup retention number of hours policy.ola.systemfullscheduled Ola\u0026rsquo;s Full System Database Backup should be scheduled $true or disabled $false policy.ola.systemintegritycheckenabled Ola\u0026rsquo;s System Database Integrity should be enabled $true or disabled $false policy.ola.systemintegritycheckscheduled Ola\u0026rsquo;s System Database Integrity should be scheduled $true or disabled $false policy.ola.userdiffenabled Ola\u0026rsquo;s Diff User Database Backup should be enabled $true or disabled $false policy.ola.userdiffretention Ola\u0026rsquo;s Diff User Database Backup retention number of hours policy.ola.userdiffscheduled Ola\u0026rsquo;s Diff User Database Backup should be scheduled $true or disabled $false policy.ola.userfullenabled Ola\u0026rsquo;s Full User Database Backup should be enabled $true or disabled $false policy.ola.userfullretention Ola\u0026rsquo;s Full User Database Backup retention number of hours policy.ola.userfullscheduled Ola\u0026rsquo;s Full User Database Backup should be scheduled $true or disabled $false policy.ola.userindexoptimizeenabled Ola\u0026rsquo;s User Index Optimization should be enabled $true or disabled $false policy.ola.userindexoptimizescheduled Ola\u0026rsquo;s User Index Optimization should be scheduled $true or disabled $false policy.ola.userintegritycheckenabled Ola\u0026rsquo;s User Database Integrity should be enabled $true or disabled $false policy.ola.userintegritycheckscheduled Ola\u0026rsquo;s User Database Integrity should be scheduled $true or disabled $false policy.ola.userlogenabled Ola\u0026rsquo;s Log User Database Backup should be enabled $true or disabled $false policy.ola.userlogretention Ola\u0026rsquo;s Log User Database Backup retention number of hours policy.ola.userlogscheduled Ola\u0026rsquo;s Log User Database Backup should be scheduled $true or disabled $false policy.oleautomation OLE Automation should be enabled $true or disabled $false policy.pageverify Page verify option should be set to this value policy.recoverymodel.excludedb Databases to exclude from standard recovery model check policy.recoverymodel.type Standard recovery model policy.storage.backuppath Enables tests to check if servers have access to centralized backup location policy.validdbowner.excludedb Databases to exclude from valid dbowner checks policy.validdbowner.name The database owner account should be this user policy.whoisactive.database Which database should contain the sp_WhoIsActive stored procedure policy.xevent.requiredrunningsession List of XE Sessions that should be running. policy.xevent.requiredstoppedsession List of XE Sessions that should not be running. policy.xevent.validrunningsession List of XE Sessions that can be be running. skip.backup.testing Don\u0026rsquo;t run Test-DbaLastBackup by default (it\u0026rsquo;s not read-only) skip.connection.ping Skip the ping check for connectivity skip.connection.remoting Skip PowerShell remoting check for connectivity skip.database.filegrowthdisabled Skip validation of datafiles which have growth value equal to zero. skip.database.logfilecounttest Skip the logfilecount test skip.datafilegrowthdisabled Skip validation of datafiles which have growth value equal to zero. skip.dbcc.datapuritycheck Skip data purity check in last good dbcc command skip.diffbackuptest Skip the Differential backup test skip.logfilecounttest Skip the logfilecount test skip.logshiptesting Skip the logshipping test skip.tempdb1118 Don\u0026rsquo;t run test for Trace Flag 1118 skip.tempdbfilecount Don\u0026rsquo;t run test for Temp Database File Count skip.tempdbfilegrowthpercent Don\u0026rsquo;t run test for Temp Database File Growth in Percent skip.tempdbfilesizemax Don\u0026rsquo;t run test for Temp Database Files Max Size skip.tempdbfilesonc Don\u0026rsquo;t run test for Temp Database Files on C So there are a lot of configurations that you can use. A lot are already set by default but all of them you can configure for the values that you need for your own estate.\nThe configurations are stored in the registry at HKCU:\\Software\\Microsoft\\WindowsPowerShell\\PSFramework\\\nFirst Configurations First I would run this so that you can see all of the configs in a seperate window (note this does not work on PowerShell v6)\nGet-DbcConfig | Out-GridView\rLets start with the first configurations that you will want to set. This should be the Instances and the Hosts that you want to check\nYou can get the value of the configuration item using\nGet-DbcConfigValue -Name app.sqlinstance\ras you can see in the image, nothing is returned so we have no instances configured at present. We have added tab completion to the name parameter so that you can easily find the right one\nIf you want to look at more information about the configuration item you can use\nGet-DbcConfig -Name app.sqlinstance\rwhich shows you the name, current value and the description\nSo lets set our first configuration for our SQL instance to localhost. I have included a video so you can see the auto-complete in action as well\nSet-DbcConfig -Name app.sqlinstance localhost\rThis configuration will be used for any SQL based checks but not for any windows based ones like Services, PowerPlan, SPN, DiskSpace, Cluster so lets set the app.computername configuration as well\nThis means that when we run invoke-DbcCheck with AllChecks or by specifying a check, it will run against the local machine and default instance unless we specify a sqlinstance when calling Invoke-DbcCheck. So the code below will not use the configuration for app.sqlinstance.\nInvoke-DbcCheck -SqlInstance TheBeard\rExclude a Check You can exclude a check using the -ExcludeCheck parameter of Invoke-DbcConfig. In the example below I am running all of the Server checks but excluding the SPN as we are not on a domain\nInvoke-DbcCheck -Check Server -ExcludeCheck SPN\rThere is a configuration setting to exclude checks as well. (Be careful this will exclude them even if you specifically specify a check using Invoke-DbcCheck but we do give you a warning!)\nSo now I can run\nSet-DbcConfig -Name command.invokedbccheck.excludecheck -Value SPN\rInvoke-DbcCheck -Check Server\rand all of the server checks except the SPN check will run against the local machine and the default instance that I have set in the config\nCreating an environment config and exporting it to use any time we like So lets make this a lot more useful. Lets create a configuration for our production environment and save it to disk (or even source control it!) so that we can use it again and again. We can also then pass it to other members of our team or even embed it in an automated process or our CI/CD system\nLets build up a configuration for a number of tests for my “production” environment. I will not explain them all here but let you read through the code and the comments to see what has been set. You will see that some of them are due to me running the test on a single machine with one drive.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # The computername we will be testing Set-DbcConfig -Name app.computername -Value localhost # The Instances we want to test Set-DbcConfig -Name app.sqlinstance -Value \u0026#39;localhost\u0026#39; ,\u0026#39;localhost\\PROD1\u0026#39;,\u0026#39;localhost\\PROD2\u0026#39;, \u0026#39;localhost\\PROD3\u0026#39; # The database owner we expect Set-DbcConfig -Name policy.validdbowner.name -Value \u0026#39;dbachecksdemo\\dbachecks\u0026#39; # the database owner we do NOT expect Set-DbcConfig -Name policy.invaliddbowner.name -Value \u0026#39;sa\u0026#39; # Should backups be compressed by default? Set-DbcConfig -Name policy.backup.defaultbackupcompreesion -Value $true # Do we allow DAC connections? Set-DbcConfig -Name policy.dacallowed -Value $true # What recovery model should we have? Set-DbcConfig -Name policy.recoverymodel.type -value FULL # What should our database growth type be? Set-DbcConfig -Name policy.database.filegrowthtype -Value kb # What authentication scheme are we expecting? Set-DbcConfig -Name policy.connection.authscheme -Value \u0026#39;NTLM\u0026#39; # Which Agent Operator should be defined? Set-DbcConfig -Name agent.dbaoperatorname -Value \u0026#39;DBA Team\u0026#39; # Which Agent Operator email should be defined? Set-DbcConfig -Name agent.dbaoperatoremail -Value \u0026#39;DBATeam@TheBeard.Local\u0026#39; # Which failsafe operator shoudl be defined? Set-DbcConfig -Name agent.failsafeoperator -Value \u0026#39;DBA Team\u0026#39; # Where is the whoisactive stored procedure? Set-DbcConfig -Name policy.whoisactive.database -Value DBAAdmin # What is the maximum time since I took a Full backup? Set-DbcConfig -Name policy.backup.fullmaxdays -Value 7 # What is the maximum time since I took a DIFF backup (in hours) ? Set-DbcConfig -Name policy.backup.diffmaxhours -Value 26 # What is the maximum time since I took a log backup (in minutes)? Set-DbcConfig -Name policy.backup.logmaxminutes -Value 30 # What is my domain name? Set-DbcConfig -Name domain.name -Value \u0026#39;WORKGROUP\u0026#39; # Where is my Ola database? Set-DbcConfig -Name policy.ola.database -Value DBAAdmin # Which database should not be checked for recovery model Set-DbcConfig -Name policy.recoverymodel.excludedb -Value \u0026#39;master\u0026#39;,\u0026#39;msdb\u0026#39;,\u0026#39;tempdb\u0026#39; # What is my SQL Credential Set-DbcConfig -Name app.sqlcredential -Value $null # Should I skip the check for temp files on c? Set-DbcConfig -Name skip.tempdbfilesonc -Value $true # Should I skip the check for temp files count? Set-DbcConfig -Name skip.tempdbfilecount -Value $true # Which Checks should be excluded? Set-DbcConfig -Name command.invokedbccheck.excludecheck -Value LogShipping,ExtendedEvent, HADR, PseudoSimple,spn # How many months before a build is unsupported do I want to fail the test? Set-DbcConfig -Name policy.build.warningwindow -Value 6 Get-Dbcconfig | ogv When I run this I get\nI can then export this to disk (to store in source control) using\nExport-DbcConfig -Path C:\\Users\\dbachecks\\Desktop\\production_config.json\rand I have a configuration file\nwhich I can use any time to set the configuration for dbachecks using the Import-DbcConfig command (But this doesn’t work in VS Codes integrated terminal – which occasionally does odd things, this appears to be one of them)\nImport-DbcConfig -Path C:\\Users\\dbachecks\\Desktop\\production_config.json\rSo I can import this configuration and run my checks with it any time I like. This means that I can create many different test configurations for my many different environment or estate configurations.\nYes, I know “good/best practice” says we should use the same configuration for all of our instances but we know that isn’t true. We have instances that were set up 15 years ago that are still in production. We have instances from the companies our organisation has bought over the years that were set up by system administrators. We have instances that were set up by shadow IT and now we have to support but cant change.\nAs well as those though, we also have different environments. Our development or test environment will have different requirements to our production environments.\nIn this hypothetical situation the four instances for four different applications have 4 development containers which are connected to using SQL Authentication. We will need a different configuration.\nSQL Authentication We can set up SQL Authentication for connecting to our SQL Instances using the app.sqlcredential configuration. this is going to hold a PSCredential object for SQL Authenticated connection to your instance. If this is set the checks will always try to use it. Yes this means that the same username and password is being used for each connection. No there is currently no way to choose which instances use it and which don’t. This may be a limitation but as you will see further down you can still do this with different configurations\nTo set the SQL Authentication run\nSet-DbcConfig -Name app.sqlcredential -Value (Get-Credential)\rThis will give a prompt for you to enter the credential\nDevelopment Environment Configuration So now we know how to set a SQL Authentication configuration we can create our development environment configuration like so. As you can see below the values are different for the checks and more checks have been skipped. I wont explain it all, if it doesn’t make sense ask a question in the comments or in the dbachecks in SQL Server Community Slack\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #region Dev Config # The Instances we want to test Set-DbcConfig -Name app.sqlinstance -Value \u0026#39;localhost,1401\u0026#39; ,\u0026#39;localhost,1402\u0026#39;,\u0026#39;localhost,1403\u0026#39;, \u0026#39;localhost,1404\u0026#39; # What is my SQL Credential Set-DbcConfig -Name app.sqlcredential -Value (Get-Credential) # The database owner we expect Set-DbcConfig -Name policy.validdbowner.name -Value \u0026#39;sa\u0026#39; # What authentication scheme are we expecting? Set-DbcConfig -Name policy.connection.authscheme -Value \u0026#39;SQL\u0026#39; # the database owner we do NOT expect Set-DbcConfig -Name policy.invaliddbowner.name -Value \u0026#39;dbachecksdemo\\dbachecks\u0026#39; # Should backups be compressed by default? Set-DbcConfig -Name policy.backup.defaultbackupcompreesion -Value $false # What should our database growth type be? Set-DbcConfig -Name policy.database.filegrowthtype -Value kb # What should our database growth value be higher than (Mb)? Set-DbcConfig -Name policy.database.filegrowthvalue -Value 64 # Do we allow DAC connections? Set-DbcConfig -Name policy.dacallowed -Value $false # What is the maximum latency (ms)? Set-DbcConfig -Name policy.network.latencymaxms -Value 100 # What recovery model should we have? Set-DbcConfig -Name policy.recoverymodel.type -value Simple # Where is the whoisactive stored procedure? Set-DbcConfig -Name policy.whoisactive.database -Value DBAAdmin # What is my domain name? Set-DbcConfig -Name domain.name -Value \u0026#39;WORKGROUP\u0026#39; # Which database should not be checked for recovery model Set-DbcConfig -Name policy.recoverymodel.excludedb -Value \u0026#39;master\u0026#39;,\u0026#39;msdb\u0026#39;,\u0026#39;tempdb\u0026#39; # Should I skip the check for temp files on c? Set-DbcConfig -Name skip.tempdbfilesonc -Value $true # Should I skip the check for temp files count? Set-DbcConfig -Name skip.tempdbfilecount -Value $true # How many months before a build is unsupported do I want to fail the test? Set-DbcConfig -Name policy.build.warningwindow -Value 6 # Which Checks should be excluded? Set-DbcConfig -Name command.invokedbccheck.excludecheck -Value LogShipping,ExtendedEvent, HADR, SaReNamed, PseudoSimple,spn, DiskSpace, DatabaseCollation,Agent,Backup,UnusedIndex,LogfileCount,FileGroupBalanced,LogfileSize,MaintenanceSolution,ServerNameMatch Export-DbcConfig -Path C:\\Users\\dbachecks\\Desktop\\development_config.json Using The Different Configurations Now I have two configurations, one for my Production Environment and one for my development environment. I can run my checks whenever I like (perhaps you will automate this in some way)\nImport the production configuration Run my tests with that configuration and create a json file for my Power Bi labelled production Import the development configuration (and enter the SQL authentication credential) Run my tests with that configuration and create a json file for my Power Bi labelled development Start Power Bi to show those results 1 2 3 4 5 6 7 8 9 10 # Import the production config Import-DbcConfig C:\\Users\\dbachecks\\Desktop\\production_config.json # Run the tests with the production config and create/update the production json Invoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment Production # Import the development config Import-DbcConfig C:\\Users\\dbachecks\\Desktop\\development_config.json # Run the tests with the production config and create/update the development json Invoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment Development # Open the PowerBi Start-DbcPowerBi I have published the Power Bi so that you can see what it would like and have a click around (maybe you can see improvements you would like to contribute)\nnow we can see how each environment is performing according to our settings for each environment\nCombining Configurations Into One Result Set As you saw above, by using the Environment parameter of Update-DbcPowerBiDataSource you can add different environments to one report. But if I wanted to have a report for my application APP1 showing both production and development environments but they have different configurations how can I do this?\nHere’s how.\nCreate a configuration for the production environment (I have used the production configuration one from above but only localhost for the instance) Export it using to C:\\Users\\dbachecks\\Desktop\\APP1-Prod_config.json Create a configuration for the development environment (I have used the development configuration one from above but only localhost,1401 for the instance) Export it using to C:\\Users\\dbachecks\\Desktop\\APP1-Dev_config.json Then run\n1 2 3 4 5 6 7 8 9 # Import the production config Import-DbcConfig C:\\Users\\dbachecks\\Desktop\\APP1-Prod_config.json # Run the tests with the production config and create/update the production json Invoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment APP1 # Import the development config Import-DbcConfig C:\\Users\\dbachecks\\Desktop\\APP1-Dev_config.json # Run the tests with the production config and create/update the development json Invoke-DbcCheck -AllChecks -Show Fails -PassThru |Update-DbcPowerBiDataSource -Environment APP1 -Append Start-DbcPowerBi Notice that this time there is an Append on the last Invoke-DbcCheck this creates a single json file for the PowerBi and the results look like this. Now we have the results for our application and both the production environment localhost and the development container localhost,1401\nIt’s Open Source – We Want Your Ideas, Issues, New Code dbachecks is open-source available on GitHub for anyone to contribute\nWe would love you to contribute. Please open issues for new tests, enhancements, bugs. Please fork the repository and add code to improve the module. please give feedback to make this module even more useful\nYou can also come in the SQL Server Community Slack and join the dbachecks channel and get advice, make comments or just join in the conversation\nThank You I want to say thank you to all of the people who have enabled dbachecks to get this far. These wonderful people have used their own time to ensure that you have a useful tool available to you for free\nChrissy Lemaire @cl\nFred Weinmann @FredWeinmann\nCláudio Silva @ClaudioESSilva\nStuart Moore @napalmgram\nShawn Melton @wsmelton\nGarry Bargsley @gbargsley\nStephen Bennett @staggerlee011\nSander Stad @SQLStad\nJess Pomfret @jpomfret\nJason Squires @js0505\nShane O’Neill @SOZDBA\nand all of the other people who have contributed in the dbachecks Slack channel\n","date":"2018-02-22T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/02/03-autocomplete.png","permalink":"https://dbachecks.io/blog/dbachecks-configuration-deep-dive/","title":"dbachecks – Configuration Deep Dive"},{"content":"Before you run a PowerShell command that makes a change to something you should check that it is going to do what you expect. You can do this by using the WhatIf parameter for commands that support it. For example, if you wanted to create a New SQL Agent Job Category you would use the awesome dbatools module and write some code like this\nNew-DbaAgentJobCategory -SqlInstance ROB-XPS -Category 'Backup'\rbefore you run it, you can check what it is going to do using\nNew-DbaAgentJobCategory -SqlInstance ROB-XPS -Category 'Backup' -WhatIf\rwhich gives a result like this\nThis makes it easy to do at the command line but when we get confident with PowerShell we will want to write scripts to perform tasks using more than one command. So how can we ensure that we can check that those will do what we are expecting without actually running the script and see what happens? Of course, there are Unit and integration testing that should be performed using Pester when developing the script but there will still be occasions when we want to see what this script will do this time in this environment.\nLets take an example. We want to place our SQL Agent jobs into specific custom categories depending on their name. We might write a script like this\n\u0026lt;#\r.SYNOPSIS\rAdds SQL Agent Jobs to categories and creates the categories if needed\r.DESCRIPTION\rAdds SQL Agent Jobs to categories and creates the categories if needed. Creates\rBackup', 'Index', 'TroubleShooting','General Info Gathering' categories and adds\rthe agent jobs depending on name to the category\r.PARAMETER Instance\rThe Instance to run the script against\r#\u0026gt;\rParam(\r[string]$Instance\r)\r$Categories = 'Backup', 'Index','DBCC', 'TroubleShooting', 'General Info Gathering'\r$Categories.ForEach{\r## Create Category if it doesnot exist\rIf (-not (Get-DbaAgentJobCategory -SqlInstance $instance -Category $PSItem)) {\rNew-DbaAgentJobCategory -SqlInstance $instance -Category $PSItem -CategoryType LocalJob\r}\r}\r## Get the agent jobs and iterate through them\r(Get-DbaAgentJob -SqlInstance $instance).ForEach{\r## Depending on the name of the Job - Put it in a Job Category\rswitch -Wildcard ($PSItem.Name) {\r'*DatabaseBackup*' { Set-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category 'Backup'\r}\r'*Index*' { Set-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category 'Index'\r}\r'*DatabaseIntegrity*' { Set-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category 'DBCC'\r}\r'*Log SP_*' { Set-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category 'TroubleShooting'\r}\r'*Collection*' { Set-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category 'General Info Gathering'\r}\r## Otherwise put it in the uncategorised category\rDefault {\rSet-DbaAgentJob -SqlInstance $instance -Job $PSItem -Category '[Uncategorized (Local)]'\r}\r}\r}\rYou can run this script against any SQL instance by calling it and passing an instance parameter from the command line like this\n\u0026amp; C:\\temp\\ChangeJobCategories.ps1 -instance ROB-XPS\rIf you wanted to see what would happen, you could edit the script and add the WhatIf parameter to every changing command but that’s not really a viable solution. What you can do is\n$PSDefaultParameterValues['*:WhatIf'] = $true\rthis will set all commands that accept WhatIf to use the WhatIf parameter. This means that if you are using functions that you have written internally you must ensure that you write your functions to use the common parameters\nOnce you have set the default value for WhatIf as above, you can simply call your script and see the WhatIf output\n\u0026amp; C:\\temp\\ChangeJobCategories.ps1 -instance ROB-XPS\rwhich will show the WhatIf output for the script\nOnce you have checked that everything is as you expected then you can remove the default value for the WhatIf parameter and run the script\n$PSDefaultParameterValues['*:WhatIf'] = $false\r\u0026amp; C:\\temp\\ChangeJobCategories.ps1 -instance ROB-XPS\rand get the expected output\nIf you wish to see the verbose output or ask for confirmation before any change you can set those default parameters like this\n## To Set Verbose output\r$PSDefaultParameterValues['*:Verbose'] = $true\r## To Set Confirm\r$PSDefaultParameterValues['*:Confirm'] = $true\rand set them back by setting to false\n","date":"2018-01-23T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/01/02-Showing-the-results.png","permalink":"https://dbachecks.io/blog/how-to-run-a-powershell-script-file-with-verbose-confirm-or-whatif/","title":"How to run a PowerShell script file with Verbose, Confirm or WhatIf"},{"content":"I was going through my demo for the South Coast User Group meeting tonight and decided to add some details about the Because parameter available in the Pester pre-release version 4.2.0.\nTo install a pre-release version you need to get the latest PowerShellGet module. This is pre-installed with PowerShell v6 but for earlier versions open PowerShell as administrator and run\nInstall-Module PowerShellGet\rYou can try out the Pester pre-release version (once you have the latest PowerShellGet) by installing it from the PowerShell Gallery with\nInstall-Module -Name Pester -AllowPrerelease -Force # -Scope CurrentUser # if not admin\rThere are a number of improvements as you can see in the change log I particularly like the\nAdd -BeTrue to test for truthy values Add -BeFalse to test for falsy values This release adds the Because parameter to the all assertions. This means that you can add a reason why the test has failed. As JAKUB JAREŠ writes here\nReasons force you think more\nReasons document your intent\nReasons make your TestCases clearer\nSo you can do something like this\nDescribe \u0026ldquo;This shows the Because\u0026rdquo;{ It \u0026ldquo;Should be true\u0026rdquo; { $false | Should -BeTrue -Because \u0026ldquo;The Beard said so\u0026rdquo; } }\nWhich gives an error message like this 🙂\nAs you can see the Expected gives the expected value and then your Because statement and then the actual result. Which means that you could write validation tests like\nDescribe \u0026quot;My System\u0026quot; {\rContext \u0026quot;Server\u0026quot; {\rIt \u0026quot;Should be using XP SP3\u0026quot; {\r(Get-CimInstance -ClassName win32_operatingsystem) .Version | Should -Be '5.1.2600' -Because \u0026quot;We have failed to bother to update the App and it only works on XP\u0026quot;\r}\rIt \u0026quot;Should be running as rob-xps\\\\mrrob\u0026quot; {\rwhoami | Should -Be 'rob-xps\\\\mrrob' -Because \u0026quot;This is the user with the permissions\u0026quot;\r}\rIt \u0026quot;Should have SMB1 enabled\u0026quot; {\r(Get-SmbServerConfiguration).EnableSMB1Protocol | Should -BeTrue -Because \u0026quot;We don't care about the risk\u0026quot;\r}\r}\r}\rand get a result like this\nOr if you were looking to validate your SQL Server you could write something like this\nIt \u0026quot;Backups Should have Succeeeded\u0026quot; {\r$Where = {$\\_IsEnabled -eq $true -and $\\_.Name -like '\\*databasebackup\\*'}\r$Should = @{\rBeTrue = $true\rBecause = \u0026quot;WE NEED BACKUPS - OMG\u0026quot;\r}\r(Get-DbaAgentJob -SqlInstance $instance| Where-Object $where).LastRunOutcome -NotContains 'Failed' | Should @Should\r}\ror maybe your security policies allow Windows Groups as logins on your SQL instances. You could easily link to the documentation and explain why this is important. This way you could build up a set of tests to validate your SQL Server is just so for your environment\nIt \u0026quot;Should only have Windows groups as logins\u0026quot; {\r$Should = @{\rBefalse = $true\rBecause = \u0026quot;Our Security Policies say we must only have Windows groups as logins - See this document\u0026quot;\r}\r(Get-DbaLogin -SqlInstance $instance -WindowsLogins). LoginType -contains 'WindowsUser' | Should @Should\r}\rJust for fun, these would look like this\nand the code looks like\n$Instances = 'Rob-XPS', 'Rob-XPS\\\\Bolton'\rforeach ($instance in $Instances) {\r$Server, $InstanceName = $Instance.Split('/')\rif ($InstanceName.Length -eq 0) {$InstanceName = 'MSSSQLSERVER'}\rDescribe \u0026quot;Testing the instance $instance\u0026quot; {\rContext \u0026quot;SQL Agent Jobs\u0026quot; {\rIt \u0026quot;Backups Should have Succeeeded\u0026quot; {\r$Where = {$\\_IsEnabled -eq $true -and $\\_. Name -like '\\*databasebackup\\*'}\r$Should = @{\rBeTrue = $true\rBecause = \u0026quot;WE NEED BACKUPS - OMG \u0026quot;\r}\r(Get-DbaAgentJob -SqlInstance $instance| Where-Object $where).LastRunOutcome -NotContains 'Failed' | Should @Should\r}\rContext \u0026quot;Logins\u0026quot; {\rIt \u0026quot;Should only have Windows groups as logins\u0026quot; {\r$Should = @{\rBefalse = $true\rBecause = \u0026quot;Our Security Policies say we must only have Windows groups as logins - See this document\u0026quot;\r}\r(Get-DbaLogin -SqlInstance $instance -WindowsLogins).LoginType -contains 'WindowsUser' | Should @Should\r}\r}\r}\r}\r}\rThis will be a useful improvement to Pester when it is released and enable you to write validation checks with explanations.\nCome and Learn Some PowerShell Magic* at #SQLBits with @cl and I\nDetails https://t.co/7OfK75e6Y1\nRegistration https://t.co/RDSkPlfMMx\n*PowerShell is not magic – it just might appear that way pic.twitter.com/5czPzYR3QD\n— Rob Sewell (@sqldbawithbeard) November 27, 2017\nChrissy has written about dbachecks the new up and coming community driven open source PowerShell module for SQL DBAs to validate their SQL Server estate. we have taken some of the ideas that we have presented about a way of using dbatools with Pester to validate that everything is how it should be and placed them into a meta data driven framework to make things easy for anyone to use. It is looking really good and I am really excited about it. It will be released very soon.\nChrissy and I will be doing a pre-con at SQLBits where we will talk in detail about how this works. You can find out more and sign up here\n","date":"2018-01-18T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/01/01-Because-1.png","permalink":"https://dbachecks.io/blog/pester-4.2.0-has-a-because-because-/","title":"Pester 4.2.0 has a Because…… because :-)"},{"content":"TagLine – My goal – Chrissy will appreciate Unit Tests one day 🙂\nChrissy has written about dbachecks the new up and coming community driven open source PowerShell module for SQL DBAs to validate their SQL Server estate. we have taken some of the ideas that we have presented about a way of using dbatools with Pester to validate that everything is how it should be and placed them into a meta data driven framework to make things easy for anyone to use. It is looking really good and I am really excited about it. It will be released very soon.\nChrissy and I will be doing a pre-con at SQLBits where we will talk in detail about how this works. You can find out more and sign up here\nCláudio Silva has improved my PowerBi For Pester file and made it beautiful and whilst we were discussing this we found that if the Pester Tests were not formatted correctly the Power Bi looked … well rubbish to be honest! Chrissy asked if we could enforce some rules for writing our Pester tests.\nThe rules were\nThe Describe title should be in double quotes\nThe Describe should use the plural Tags parameter\nThe Tags should be singular\nThe first Tag should be a unique tag in Get-DbcConfig\nThe context title should end with $psitem\nThe code should use Get-SqlInstance or Get-ComputerName\nThe Code should use the forEach method\nThe code should not use $_\nThe code should contain a Context block\nShe asked me if I could write the Pester Tests for it and this is how I did it. I needed to look at the Tags parameter for the Describe. It occurred to me that this was a job for the Abstract Syntax Tree (AST). I don’t know very much about the this but I sort of remembered reading a blog post by Francois-Xavier Cat about using it with Pester so I went and read that and found an answer on Stack Overflow as well. These looked just like what I needed so I made use of them. Thank you very much to Francois-Xavier and wOxxOm for sharing.\nThe first thing I did was to get the Pester Tests which we have located in a checks folder and loop through them and get the content of the file with the Raw parameter\nContext \u0026quot;$($_.Name) - Checking Describes titles and tags\u0026quot; {\rThen I decided to look at the Describes using the method that wOxxOm (I know no more about this person!) showed.\n$Describes = \\[Management.Automation.Language.Parser\\] ::ParseInput($check, \\[ref\\]$tokens, \\[ref\\]$errors).\rFindAll(\\[Func\\[Management.Automation.Language.Ast, bool\\]\\] {\rparam($ast)\r$ast.CommandElements -and\r$ast.CommandElements\\[0\\].Value -eq 'describe'\r}, $true) |\rForEach {\r$CE = $_.CommandElements\r$secondString = ($CE |Where { $_.StaticType.name -eq 'string' })\\[1\\]\r$tagIdx = $CE.IndexOf(($CE |Where ParameterName -eq'Tags') ) + 1\r$tags = if ($tagIdx -and $tagIdx -lt $CE.Count) {\r$CE\\[$tagIdx\\].Extent\r}\rNew-Object PSCustomObject -Property @{\rName = $secondString\rTags = $tags\r}\r}\rAs I understand it, this code is using the Parser on the $check (which contains the code from the file) and finding all of the Describe commands and creating an object of the title of the Describe with the StaticType equal to String and values from the Tag parameter.\nWhen I ran this against the database tests file I got the following results\nThen it was a simple case of writing some tests for the values\n@($describes).Foreach{\r$title = $PSItem.Name.ToString().Trim('\u0026quot;').Trim('''')\rIt \u0026quot;$title Should Use a double quote after the Describe\u0026quot; {\r$PSItem.Name.ToString().Startswith('\u0026quot;')| Should be $true\r$PSItem.Name.ToString().Endswith('\u0026quot;')| Should be $true\r}\rIt \u0026quot;$title should use a plural for tags\u0026quot; {\r$PsItem.Tags| Should Not BeNullOrEmpty\r}\r# a simple test for no esses apart from statistics and Access!!\rif ($null -ne $PSItem.Tags) {\r$PSItem.Tags.Text.Split(',').Trim().Where{($_ -ne '$filename') -and ($_ -notlike '\\*statistics\\*') -and ($_ -notlike '\\*BackupPathAccess\\*') }.ForEach{\rIt \u0026quot;$PsItem Should Be Singular\u0026quot; {\r$_.ToString().Endswith('s')| Should Be $False\r}\r}\rIt \u0026quot;The first Tag Should Be in the unique Tags returned from Get-DbcCheck\u0026quot; {\r$UniqueTags -contains $PSItem.Tags.Text.Split(',') \\[0\\].ToString()| Should Be $true\r}\r}\relse {\rIt \u0026quot;You haven't used the Tags Parameter so we can't check the tags\u0026quot; {\r$false| Should be $true\r}\r}\r}\rThe Describes variable is inside @() so that if there is only one the ForEach Method will still work. The unique tags are returned from our command Get-DbcCheck which shows all of the checks. We will have a unique tag for each test so that they can be run individually.\nYes, I have tried to ensure that the tags are singular by ensuring that they do not end with an s (apart from statistics) and so had to not check BackupPathAccess and statistics. Filename is a variable that we add to each Describe Tags so that we can run all of the tests in one file. I added a little if block to the Pester as well so that the error if the Tags parameter was not passed was more obvious\nI did the same with the context blocks as well\nContext \u0026quot;$($_.Name) - Checking Contexts\u0026quot; {\r## Find the Contexts\r$Contexts = \\[Management.Automation.Language.Parser\\] ::ParseInput($check, \\[ref\\]$tokens, \\[ref\\]$errors).\rFindAll(\\[Func\\[Management.Automation.Language.Ast, bool\\] \\] {\rparam($ast)\r$ast.CommandElements -and\r$ast.CommandElements\\[0\\].Value -eq 'Context'\r}, $true) |\rForEach {\r$CE = $_.CommandElements\r$secondString = ($CE |Where { $_.StaticType.name -eq 'string' })\\[1\\]\rNew-Object PSCustomObject -Property @{\rName = $secondString\r}\r}\r@($Contexts).ForEach{\r$title = $PSItem.Name.ToString().Trim('\u0026quot;').Trim('''')\rIt \u0026quot;$Title Should end with `$psitem So that the PowerBi will work correctly\u0026quot; {\r$PSItem.Name.ToString().Endswith('psitem\u0026quot;')| Should Be $true\r}\r}\r}\rThis time we look for the Context command and ensure that the string value ends with psitem as the PowerBi parses the last value when creating columns\nFinally I got all of the code and check if it matches some coding standards\nContext \u0026quot;$($_.Name) - Checking Code\u0026quot; {\r## This just grabs all the code\r$AST = \\[System.Management.Automation.Language.Parser\\] ::ParseInput($Check, \\[ref\\]$null, \\[ref\\]$null)\r$Statements = $AST.EndBlock.statements.Extent\r## Ignore the filename line\r@($Statements.Where{$_.StartLineNumber -ne 1}).ForEach{\r$title = \\[regex\\]::matches($PSItem.text, \u0026quot;Describe(. *)-Tag\u0026quot;).groups\\[1\\].value.Replace('\u0026quot;', '').Replace ('''', '').trim()\rIt \u0026quot;$title Should Use Get-SqlInstance or Get-ComputerName\u0026quot; {\r($PSItem.text -Match 'Get-SqlInstance') -or ($psitem.text -match 'Get-ComputerName')| Should be $true\r}\rIt \u0026quot;$title Should use the ForEach Method\u0026quot; {\r($Psitem.text -match 'Get-SqlInstance\\\\).ForEach {') -or ($Psitem.text -match 'Get-ComputerName\\\\). ForEach{')| Should Be $true# use the \\ to escape the )\r}\rIt \u0026quot;$title Should not use `$_\u0026quot; {\r($Psitem.text -match '$_')| Should Be $false\r}\rIt \u0026quot;$title Should Contain a Context Block\u0026quot; {\r$Psitem.text -match 'Context'| Should Be $True\r}\r}\rI trim the title from the Describe block so that it is easy to see where the failures (or passes) are with some regex and then loop through each statement apart from the first line to ensure that the code is using our internal commands Get-SQLInstance or Get-ComputerName to get information, that we are looping through each of those arrays using the ForEach method rather than ForEach-Object and using $psitem rather than $_ to reference the “This Item” in the array and that each Describe block has a context block.\nThis should ensure that any new tests that are added to the module follow the guidance we have set up on the Wiki and ensure that the Power Bi results still look beautiful!\nAnyone can run the tests using\nInvoke-Pester .\\\\tests\\\\Unit.Tests.ps1 -show Fails\rbefore they create a Pull request and it looks like\nif everything is Green then they can submit their Pull Request 🙂 If not they can see quickly that something needs to be fixed. (fail early 🙂 )\n","date":"2018-01-15T00:00:00Z","image":"https://dbachecks.io/assets/uploads/2018/01/02-Pester-results-1.png","permalink":"https://dbachecks.io/blog/using-the-ast-in-pester-for-dbachecks/","title":"Using the AST in Pester for dbachecks"}]